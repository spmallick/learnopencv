{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Web Scraper",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCQYDTmNvGHq"
      },
      "source": [
        "# Installations and Imports\n",
        "!pip install pillow \n",
        "!pip install selenium\n",
        "# https://github.com/AndrewCarterUK/pascal-voc-writer\n",
        "!pip install pascal-voc-writer\n",
        "# https://github.com/gereleth/jupyter-bbox-widget\n",
        "!pip install jupyter_bbox_widget\n",
        "# https://github.com/elisemercury/Duplicate-Image-Finder\n",
        "!pip install difPy\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import sys\n",
        "import PIL\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import io\n",
        "import hashlib\n",
        "from jupyter_bbox_widget import BBoxWidget\n",
        "import ipywidgets as widgets\n",
        "import json\n",
        "import base64\n",
        "from IPython.display import Image \n",
        "from pascal_voc_writer import Writer\n",
        "from google.colab import files, output, drive\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from difPy import dif\n",
        "\n",
        "# https://stackoverflow.com/questions/51046454/how-can-we-use-selenium-webdriver-in-colab-research-google-com\n",
        "output.enable_custom_widget_manager()\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiId-MmlsgJP"
      },
      "source": [
        "# Just run this cell, no need to change anything here\n",
        "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(sleep_between_interactions)    \n",
        "    \n",
        "    # build the google query\n",
        "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
        "  \n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "    print(\"page loaded\")\n",
        "    image_urls = set()\n",
        "    image_count = 0\n",
        "    results_start = 0\n",
        "    while image_count < max_links_to_fetch:\n",
        "        scroll_to_end(wd)\n",
        "\n",
        "        # get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements(By.CSS_SELECTOR,\"img.Q4LuWd\")\n",
        "        # thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "        \n",
        "        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n",
        "        \n",
        "        for img in thumbnail_results[results_start:number_results]:\n",
        "            # try to click every thumbnail such that we can get the real image behind it\n",
        "            try:\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # extract image urls    \n",
        "            actual_images = wd.find_elements(By.CSS_SELECTOR, 'img.n3VNCb')\n",
        "            for actual_image in actual_images:\n",
        "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
        "                    image_urls.add(actual_image.get_attribute('src'))\n",
        "\n",
        "            image_count = len(image_urls)\n",
        "\n",
        "            if len(image_urls) >= max_links_to_fetch:\n",
        "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
        "                break\n",
        "        else:\n",
        "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
        "            time.sleep(30)\n",
        "            return\n",
        "            load_more_button = wd.find_elements(By.CSS_SELECTOR, \".mye4qd\")\n",
        "            if load_more_button:\n",
        "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "\n",
        "        # move the result startpoint further down\n",
        "        results_start = len(thumbnail_results)\n",
        "\n",
        "    return image_urls\n",
        "\n",
        "def persist_image(folder_path:str, url:str):\n",
        "    try:\n",
        "        image_content = requests.get(url).content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not download {url} - {e}\")\n",
        "\n",
        "    try:\n",
        "        image_file = io.BytesIO(image_content)\n",
        "        image = PIL.Image.open(image_file).convert('RGB')\n",
        "        if os.path.exists(folder_path):\n",
        "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
        "        else:\n",
        "            os.mkdir(folder_path)\n",
        "            file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
        "        with open(file_path, 'wb') as f:\n",
        "            image.save(f, \"JPEG\", quality=100)\n",
        "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR - Could not save {url} - {e}\")\n",
        "\n",
        "def search_and_download(search_term:str,number_images=5, target_path='./images'):\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "    res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.1)\n",
        "        \n",
        "    for elem in res:\n",
        "        persist_image(target_path, elem)\n",
        "\n",
        "!mkdir \"/content/annotations\"\n",
        "\n",
        "def encode_image(filepath, image_path='/content/images/'):\n",
        "    filepath = image_path + filepath\n",
        "    with open(filepath, 'rb') as f:\n",
        "        image_bytes = f.read()\n",
        "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
        "    return \"data:image/jpg;base64,\"+encoded\n",
        "\n",
        "# when Skip button is pressed we move on to the next file\n",
        "def on_skip(image_path='/content/images'):\n",
        "  try:\n",
        "    w_progress.value += 1\n",
        "    # open new image in the widget\n",
        "    image_file = files[w_progress.value]\n",
        "    w_bbox.image = encode_image(image_file)\n",
        "    # here we assign an empty list to bboxes but \n",
        "    # we could also run a detection model on the file\n",
        "    # and use its output for creating inital bboxes\n",
        "    w_bbox.bboxes = [] \n",
        "  except IndexError:\n",
        "    print(\"No more images to annotate!\")\n",
        "\n",
        "  # delete skipped image\n",
        "  !rm {image_path}/{files[w_progress.value-1]}\n",
        "\n",
        "# when Submit button is pressed we save current annotations\n",
        "# and then move on to the next file\n",
        "def on_submit(image_path='/content/images/', Save_PASCAL_VOC=True):\n",
        "    image_file = files[w_progress.value]\n",
        "    # save annotations for current image\n",
        "    annotations[image_file] = w_bbox.bboxes\n",
        "    # print(annotations)\n",
        "    # print(annotations[image_file])\n",
        "    if Save_PASCAL_VOC == True:\n",
        "      save_pascal_voc_format(image_path, image_file)\n",
        "\n",
        "    with open(annotations_path, 'w') as f:\n",
        "        json.dump(annotations, f, indent=4)\n",
        "    try:\n",
        "      w_progress.value += 1\n",
        "      # open new image in the widget\n",
        "      image_file = files[w_progress.value]\n",
        "      w_bbox.image = encode_image(image_file)\n",
        "      # here we assign an empty list to bboxes but \n",
        "      # we could also run a detection model on the file\n",
        "      # and use its output for creating inital bboxes\n",
        "      w_bbox.bboxes = [] \n",
        "    except IndexError:\n",
        "      print(\"No more images to annotate!\")\n",
        "\n",
        "def save_pascal_voc_format(image_path, image_file):\n",
        "    image = PIL.Image.open(image_path + image_file)\n",
        "    width, height = image.size\n",
        "      \n",
        "    writer = Writer(image_path + image_file, width, height)\n",
        "\n",
        "    for annotation in annotations[image_file]:\n",
        "      writer.addObject(annotation['label'], annotation['x'], annotation['y'], annotation['x'] + annotation['width'], annotation['y'] + annotation['height'])\n",
        "\n",
        "    image_name = image_file.split('.')[0]\n",
        "    writer.save(f'/content/annotations/{image_name}.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to2wbF14wGBs"
      },
      "source": [
        "search_term = input(\"Enter google search: \")\n",
        "# The number of images to download\n",
        "number_images = 20\n",
        "search_and_download(search_term, number_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z6sEkKPBNfW"
      },
      "source": [
        "# Remove Duplicate Images\n",
        "dif.compare_images(\"/content/images/\", delete=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01yI167w5NYz"
      },
      "source": [
        "# Open Annotation Tool\n",
        "files = os.listdir('/content/images/')\n",
        "\n",
        "annotations = {}\n",
        "annotations_path = 'annotations.json'\n",
        "# a progress bar to show how far we got\n",
        "w_progress = widgets.IntProgress(value=0, max=len(files), description='Progress')\n",
        "\n",
        "# the bbox widget\n",
        "w_bbox = BBoxWidget(\n",
        "    image = encode_image(files[0]),\n",
        "    classes=['plastic', 'metal', 'paper', 'glass'] # Add, Remove or change the classes according to your problem\n",
        ")\n",
        "\n",
        "# combine widgets into a container\n",
        "w_container = widgets.VBox([\n",
        "    w_progress,\n",
        "    w_bbox,\n",
        "])\n",
        "\n",
        "# Caution: Skipping an image will delete it\n",
        "w_bbox.on_skip(on_skip)\n",
        "w_bbox.on_submit(on_submit)\n",
        "\n",
        "w_container"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BsBvjPTnU6A"
      },
      "source": [
        "# Use this code to save your dataset in Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# Enter the name of your dataset folder or leave default\n",
        "dataset_folder=\"MyDataset\"\n",
        "!mkdir /content/drive/My\\ Drive/{dataset_folder}\n",
        "!cp -r /content/images /content/drive/My\\ Drive/$dataset_folder\n",
        "!cp -r /content/annotations /content/drive/My\\ Drive/$dataset_folder\n",
        "!cat /content/annotations.json >> /content/drive/My\\ Drive/$dataset_folder/annotations.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM_bPY1OwhAE"
      },
      "source": [
        "# Delete images and annotations after saving to google drive\n",
        "# Use before downloading more images to annotate\n",
        "!rm -rf /content/images\n",
        "!rm -rf /content/annotations\n",
        "!rm /content/annotations.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3396-SVHW-yK"
      },
      "source": [
        "Special thanks for the web scraping code to \n",
        "*   https://towardsdatascience.com/image-scraping-with-python-a96feda8af2d\n",
        "*   https://medium.com/@wwwanandsuresh/web-scraping-images-from-google-9084545808a2"
      ]
    }
  ]
}