{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_Pytorch_Fashion-MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6zXv_t2jXEa"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nln2VA_bjYsy"
      },
      "source": [
        "!mkdir diff-run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D54v_DF0jakG"
      },
      "source": [
        "!mkdir diff-run/images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoj6U2rmjiS-"
      },
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "import datetime"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S2ZtWdajpz3",
        "outputId": "dce7d573-ddc8-4e5d-8f8f-d791fe517150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4b9d4f0750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56IrzS6xmqTk"
      },
      "source": [
        "writer = SummaryWriter('diff-run/py-gan')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVXC_q2ekuf8"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 128"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmKRUtX6kwt1"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])])\n",
        "train_dataset = datasets.FashionMNIST(root='./data/', train=True, transform=train_transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ImLVM7lk2du"
      },
      "source": [
        "image_shape = (1, 28, 28)\n",
        "image_dim = int(np.prod(image_shape))\n",
        "latent_dim = 100"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gw3SMN7jtOB"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(nn.Linear(latent_dim, 128),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "        \t\t\t\t\t\t\tnn.Linear(128, 256),\n",
        "                      nn.BatchNorm1d(256, 0.8),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True), \n",
        "        \t\t\t\t\t\t\tnn.Linear(256, 512),\n",
        "                      nn.BatchNorm1d(512, 0.8),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True), \n",
        "                      nn.Linear(512, 1024),\n",
        "                      nn.BatchNorm1d(1024, 0.8),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True), \n",
        "        \t\t\t\t\t\t\tnn.Linear(1024, image_dim),\n",
        "        \t\t\t\t\t\t\tnn.Tanh())\n",
        "    \n",
        "    def forward(self, noise_vector): \n",
        "        image = self.model(noise_vector)\n",
        "        image = image.view(image.size(0), *image_shape)\n",
        "        return image\n",
        "\n",
        "\n",
        "   "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPEMXbaJCPsQ"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(nn.Linear(image_dim, 512),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "        \t\t\t\t\t\t\tnn.Linear(512, 256),\n",
        "        \t\t\t\t\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
        "        \t\t\t\t\t\t\tnn.Linear(256, 1),\n",
        "        \t\t\t\t\t\t\tnn.Sigmoid())\n",
        "    \n",
        "    def forward(self, image):\n",
        "        image_flattened = image.view(image.size(0), -1)\n",
        "        result = self.model(image_flattened)\n",
        "        return result     "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HJv-CnSkIuN"
      },
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RluLBy9t_7X"
      },
      "source": [
        "#torch.save(generator.state_dict(), 'generator.pth')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP2GVHfYFdQt"
      },
      "source": [
        "# for layer in generator.children():\n",
        "#     print(layer.type)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-3o5-nqM_m3",
        "outputId": "1bc5bbc2-5366-435b-f65c-fb778df4319e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        }
      },
      "source": [
        "summary(generator, (100,))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 128]          12,928\n",
            "         LeakyReLU-2                  [-1, 128]               0\n",
            "            Linear-3                  [-1, 256]          33,024\n",
            "       BatchNorm1d-4                  [-1, 256]             512\n",
            "         LeakyReLU-5                  [-1, 256]               0\n",
            "            Linear-6                  [-1, 512]         131,584\n",
            "       BatchNorm1d-7                  [-1, 512]           1,024\n",
            "         LeakyReLU-8                  [-1, 512]               0\n",
            "            Linear-9                 [-1, 1024]         525,312\n",
            "      BatchNorm1d-10                 [-1, 1024]           2,048\n",
            "        LeakyReLU-11                 [-1, 1024]               0\n",
            "           Linear-12                  [-1, 784]         803,600\n",
            "             Tanh-13                  [-1, 784]               0\n",
            "================================================================\n",
            "Total params: 1,510,032\n",
            "Trainable params: 1,510,032\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.05\n",
            "Params size (MB): 5.76\n",
            "Estimated Total Size (MB): 5.82\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdNrhNT7NcQ4",
        "outputId": "3e1c4c02-d689-4f0e-a055-cc7ead6bafb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "summary(discriminator, (1,28,28))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 512]         401,920\n",
            "         LeakyReLU-2                  [-1, 512]               0\n",
            "            Linear-3                  [-1, 256]         131,328\n",
            "         LeakyReLU-4                  [-1, 256]               0\n",
            "            Linear-5                    [-1, 1]             257\n",
            "           Sigmoid-6                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 2.04\n",
            "Estimated Total Size (MB): 2.05\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFxQC7T0laZi"
      },
      "source": [
        "adversarial_loss = nn.BCELoss() "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sis4zEVQkLf_"
      },
      "source": [
        "learning_rate = 0.0002 \n",
        "G_optimizer = optim.Adam(generator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(discriminator.parameters(), lr = learning_rate, betas=(0.5, 0.999))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWjUcjslt_2F"
      },
      "source": [
        "cuda = True if torch.cuda.is_available() else False\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQLFEfpgkLjS"
      },
      "source": [
        "num_epochs = 500\n",
        "D_loss_plot, G_loss_plot = [], []\n",
        "for epoch in range(1, num_epochs+1): \n",
        "\n",
        "    D_loss_list, G_loss_list = [], []\n",
        "   \n",
        "    for index, (real_images, _) in enumerate(train_loader):\n",
        "      D_optimizer.zero_grad()\n",
        "      real_images = real_images.to(device)\n",
        "      real_target = Variable(torch.ones(real_images.size(0), 1).to(device))\n",
        "      fake_target = Variable(torch.zeros(real_images.size(0), 1).to(device))\n",
        "\n",
        "      D_real_loss = adversarial_loss(discriminator(real_images), real_target)\n",
        "      # print(discriminator(real_images))\n",
        "\n",
        "      noise_vector = Variable(torch.randn(real_images.size(0), latent_dim).to(device))\n",
        "      #noise_vector = Variable(Tensor(np.random.normal(0, 1, \\\n",
        "      #                                                (real_images.size(0),\\\n",
        "      #                                                 latent_dim))))\n",
        "      noise_vector = noise_vector.to(device)\n",
        "      generated_image = generator(noise_vector)\n",
        "\n",
        "      D_fake_loss = adversarial_loss(discriminator(generated_image),\\\n",
        "                                     fake_target)\n",
        "\n",
        "      D_total_loss = D_real_loss + D_fake_loss\n",
        "      D_loss_list.append(D_total_loss)\n",
        "      D_total_loss.backward()\n",
        "      D_optimizer.step()\n",
        "\n",
        "      G_optimizer.zero_grad()\n",
        "      generated_image = generator(noise_vector)\n",
        "      G_loss = adversarial_loss(discriminator(generated_image), real_target)\n",
        "      G_loss_list.append(G_loss)\n",
        "\n",
        "      G_loss.backward()\n",
        "      G_optimizer.step()\n",
        "      d = generated_image.data\n",
        "      \n",
        "      writer.add_scalar('Discriminator Loss',\n",
        "                            D_total_loss,\n",
        "                            epoch * len(train_loader) + index)\n",
        "      \n",
        "      writer.add_scalar('Generator Loss',\n",
        "                            G_loss,\n",
        "                            epoch * len(train_loader) + index)\n",
        "\n",
        "\n",
        "    print('Epoch: [%d/%d]: D_loss: %.3f, G_loss: %.3f' % (\n",
        "            (epoch), num_epochs, torch.mean(torch.FloatTensor(D_loss_list)),\\\n",
        "             torch.mean(torch.FloatTensor(G_loss_list))))\n",
        "    \n",
        "    D_loss_plot.append(torch.mean(torch.FloatTensor(D_loss_list)))\n",
        "    G_loss_plot.append(torch.mean(torch.FloatTensor(G_loss_list)))\n",
        "    save_image(generated_image.data[:90], 'diff-run/images/sample_%d'%epoch + '.png', nrow=10, normalize=True)\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}