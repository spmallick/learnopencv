{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/spmallick/learnopencv/blob/master/PyTorch-Multi-Label-Image-Classification/Multi_Output_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvY4DnV8fh2n"
   },
   "source": [
    "# Setup connection with Kaggle and download the data.\n",
    "\n",
    "Please go to Kaggle and create API key to download the data. Foloow [this guide](https://www.kaggle.com/general/74235) to do it.\n",
    "\n",
    "The code in the cells below will prompt you for the `kaggle.json` file you've downloaded from the Kaggle and do the rest of the connection establishing and data downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBBfnSMzXnCx"
   },
   "outputs": [],
   "source": [
    "! pip install -q kaggle\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "\n",
    "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
    "  files.upload()\n",
    "  ! mkdir -p ~/.kaggle\n",
    "  ! mv kaggle.json ~/.kaggle/ \n",
    "  ! chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZtFAMdlhz-u"
   },
   "source": [
    "Check setup was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "6P80lAGWhy5W",
    "outputId": "a4f5d373-e6c3-4cee-905e-ce16b88019ba"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tUAqAMtRh7Xw"
   },
   "source": [
    "Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "d5Og90EZhAwc",
    "outputId": "ef291c25-7229-4287-e499-e8d2864956e2"
   },
   "outputs": [],
   "source": [
    "! kaggle datasets download paramaggarwal/fashion-product-images-small\n",
    "! mkdir -p fashion-product-images && cd fashion-product-images && unzip -qo ../fashion-product-images-small.zip && cd ..;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Of8bgjI9Juxy"
   },
   "source": [
    "# Dataset preparation\n",
    "\n",
    "In total, we are going to use 40 000 images. We’ll put 32 000 of them into the training set, and the rest 8 000 we’ll use for the validation. To split the data, run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BKJSpLTjcZac",
    "outputId": "29d89d7c-7690-4925-d9a5-8885960526e1"
   },
   "outputs": [],
   "source": [
    "# from split_data.py\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def save_csv(data, path, fieldnames=['image_path', 'gender', 'articleType', 'baseColour']):\n",
    "    with open(path, 'w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(dict(zip(fieldnames, row)))\n",
    "\n",
    "\n",
    "def split_data():\n",
    "    input_folder = \"fashion-product-images\"\n",
    "    output_folder = \"fashion-product-images\"\n",
    "    annotation = os.path.join(input_folder, 'styles.csv')\n",
    "\n",
    "    # open annotation file\n",
    "    all_data = []\n",
    "    with open(annotation) as csv_file:\n",
    "        # parse it as CSV\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        # tqdm shows pretty progress bar\n",
    "        # each row in the CSV file corresponds to the image\n",
    "        for row in tqdm(reader, total=reader.line_num):\n",
    "            # we need image ID to build the path to the image file\n",
    "            img_id = row['id']\n",
    "            # we're going to use only 3 attributes\n",
    "            gender = row['gender']\n",
    "            articleType = row['articleType']\n",
    "            baseColour = row['baseColour']\n",
    "            img_name = os.path.join(input_folder, 'images', str(img_id) + '.jpg')\n",
    "            # check if file is in place\n",
    "            if os.path.exists(img_name):\n",
    "                # check if the image has 80*60 pixels with 3 channels\n",
    "                img = Image.open(img_name)\n",
    "                if img.size == (60, 80) and img.mode == \"RGB\":\n",
    "                    all_data.append([img_name, gender, articleType, baseColour])\n",
    "\n",
    "    # set the seed of the random numbers generator, so we can reproduce the results later\n",
    "    np.random.seed(42)\n",
    "    # construct a Numpy array from the list\n",
    "    all_data = np.asarray(all_data)\n",
    "    # Take 40000 samples in random order\n",
    "    inds = np.random.choice(40000, 40000, replace=False)\n",
    "    # split the data into train/val and save them as csv files\n",
    "    save_csv(all_data[inds][:32000], os.path.join(output_folder, 'train.csv'))\n",
    "    save_csv(all_data[inds][32000:40000], os.path.join(output_folder, 'val.csv'))\n",
    "\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCNlWgE_KEMG"
   },
   "source": [
    "# Working with the dataset from code\n",
    "\n",
    "As we have more than one label in our data annotation, we need to tweak the way we read the data and load it in the memory. To do that, we’ll create a class that inherits PyTorch Dataset. It will be able to parse our data annotation and extract only the labels of our interest. The key difference between the multi-output and single-class classification is that we will return several labels per each sample from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cf1zmlRvcAq3"
   },
   "outputs": [],
   "source": [
    "# from dataset.py\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class AttributesDataset():\n",
    "    def __init__(self, annotation_path):\n",
    "        color_labels = []\n",
    "        gender_labels = []\n",
    "        article_labels = []\n",
    "\n",
    "        with open(annotation_path) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                color_labels.append(row['baseColour'])\n",
    "                gender_labels.append(row['gender'])\n",
    "                article_labels.append(row['articleType'])\n",
    "\n",
    "        self.color_labels = np.unique(color_labels)\n",
    "        self.gender_labels = np.unique(gender_labels)\n",
    "        self.article_labels = np.unique(article_labels)\n",
    "\n",
    "        self.num_colors = len(self.color_labels)\n",
    "        self.num_genders = len(self.gender_labels)\n",
    "        self.num_articles = len(self.article_labels)\n",
    "\n",
    "        self.color_id_to_name = dict(zip(range(len(self.color_labels)), self.color_labels))\n",
    "        self.color_name_to_id = dict(zip(self.color_labels, range(len(self.color_labels))))\n",
    "\n",
    "        self.gender_id_to_name = dict(zip(range(len(self.gender_labels)), self.gender_labels))\n",
    "        self.gender_name_to_id = dict(zip(self.gender_labels, range(len(self.gender_labels))))\n",
    "\n",
    "        self.article_id_to_name = dict(zip(range(len(self.article_labels)), self.article_labels))\n",
    "        self.article_name_to_id = dict(zip(self.article_labels, range(len(self.article_labels))))\n",
    "\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, annotation_path, attributes, transform=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "        self.attr = attributes\n",
    "\n",
    "        # initialize the arrays to store the ground truth labels and paths to the images\n",
    "        self.data = []\n",
    "        self.color_labels = []\n",
    "        self.gender_labels = []\n",
    "        self.article_labels = []\n",
    "\n",
    "        # read the annotations from the CSV file\n",
    "        with open(annotation_path) as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                self.data.append(row['image_path'])\n",
    "                self.color_labels.append(self.attr.color_name_to_id[row['baseColour']])\n",
    "                self.gender_labels.append(self.attr.gender_name_to_id[row['gender']])\n",
    "                self.article_labels.append(self.attr.article_name_to_id[row['articleType']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # take the data sample by its index\n",
    "        img_path = self.data[idx]\n",
    "\n",
    "        # read image\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        # apply the image augmentations if needed\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # return the image and all the associated labels\n",
    "        dict_data = {\n",
    "            'img': img,\n",
    "            'labels': {\n",
    "                'color_labels': self.color_labels[idx],\n",
    "                'gender_labels': self.gender_labels[idx],\n",
    "                'article_labels': self.article_labels[idx]\n",
    "            }\n",
    "        }\n",
    "        return dict_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-N0hE0xKSq_"
   },
   "source": [
    "# Model description\n",
    "\n",
    "Have a look into the model class definition. We take mobilenet_v2 network from torchvision.models. This model can solve the ImageNet classification, so its last layer is a single classifier.\n",
    "\n",
    "To use this model for our multi-output task, we will modify it. We need to predict three properties, so we’ll use three new classification heads instead of a single classifier: these heads are called color, gender and article. Each head will have its own cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wsub0GAhcRI4"
   },
   "outputs": [],
   "source": [
    "# from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class MultiOutputModel(nn.Module):\n",
    "    def __init__(self, n_color_classes, n_gender_classes, n_article_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = models.mobilenet_v2().features  # take the model without classifier\n",
    "        last_channel = models.mobilenet_v2().last_channel  # size of the layer before classifier\n",
    "\n",
    "        # the input for the classifier should be two-dimensional, but we will have\n",
    "        # [batch_size, channels, width, height]\n",
    "        # so, let's do the spatial averaging: reduce width and height to 1\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # create separate classifiers for our outputs\n",
    "        self.color = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_color_classes)\n",
    "        )\n",
    "        self.gender = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_gender_classes)\n",
    "        )\n",
    "        self.article = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=last_channel, out_features=n_article_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # reshape from [batch, channels, 1, 1] to [batch, channels] to put it into classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return {\n",
    "            'color': self.color(x),\n",
    "            'gender': self.gender(x),\n",
    "            'article': self.article(x)\n",
    "        }\n",
    "\n",
    "    def get_loss(self, net_output, ground_truth):\n",
    "        color_loss = F.cross_entropy(net_output['color'], ground_truth['color_labels'])\n",
    "        gender_loss = F.cross_entropy(net_output['gender'], ground_truth['gender_labels'])\n",
    "        article_loss = F.cross_entropy(net_output['article'], ground_truth['article_labels'])\n",
    "        loss = color_loss + gender_loss + article_loss\n",
    "        return loss, {'color': color_loss, 'gender': gender_loss, 'article': article_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6f5WAtmJKrSz"
   },
   "source": [
    "# Training\n",
    "\n",
    "Let's define helper function to calculate the metrics of our model during training / testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJSUgOXFK5q0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "\n",
    "def calculate_metrics(output, target):\n",
    "    _, predicted_color = output['color'].cpu().max(1)\n",
    "    gt_color = target['color_labels'].cpu()\n",
    "\n",
    "    _, predicted_gender = output['gender'].cpu().max(1)\n",
    "    gt_gender = target['gender_labels'].cpu()\n",
    "\n",
    "    _, predicted_article = output['article'].cpu().max(1)\n",
    "    gt_article = target['article_labels'].cpu()\n",
    "\n",
    "    with warnings.catch_warnings():  # sklearn may produce a warning when processing zero row in confusion matrix\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy_color = accuracy_score(y_true=gt_color.numpy(), y_pred=predicted_color.numpy())\n",
    "        accuracy_gender = accuracy_score(y_true=gt_gender.numpy(), y_pred=predicted_gender.numpy())\n",
    "        accuracy_article = accuracy_score(y_true=gt_article.numpy(), y_pred=predicted_article.numpy())\n",
    "\n",
    "    return accuracy_color, accuracy_gender, accuracy_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb_3ty4rSLrh"
   },
   "source": [
    "Let's define helper function to show image grid with labels. It helps understand our data better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h54m7D99STxs"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_gt_data(dataset, attributes):\n",
    "    imgs = []\n",
    "    gt_labels = []\n",
    "\n",
    "    n_cols = 5\n",
    "    n_rows = 3\n",
    "\n",
    "    # store the original transforms from the dataset\n",
    "    transforms = dataset.transform\n",
    "    # and not use them during visualization\n",
    "    dataset.transform = None\n",
    "\n",
    "    for img_idx in range(n_cols * n_rows):\n",
    "        sample = dataset[img_idx]\n",
    "        img = sample['img']\n",
    "        labels = sample['labels']\n",
    "        gt_color = attributes.color_id_to_name[labels['color_labels']]\n",
    "        gt_gender = attributes.gender_id_to_name[labels['gender_labels']]\n",
    "        gt_article = attributes.article_id_to_name[labels['article_labels']]\n",
    "        \n",
    "        imgs.append(img)\n",
    "        gt_labels.append(\"{}\\n{}\\n{}\".format(gt_gender, gt_article, gt_color))\n",
    "\n",
    "    title = \"Ground truth labels\"\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax, label in zip(imgs, axs, gt_labels):\n",
    "        ax.set_xlabel(label, rotation=0)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.imshow(img)\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # restore original transforms\n",
    "    dataset.transform = transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHRi94HQmgYS"
   },
   "source": [
    "To estimate the accuracy of the model during training we will need the validation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdf46H7QmmcY"
   },
   "outputs": [],
   "source": [
    "def checkpoint_load(model, name):\n",
    "    print('Restoring checkpoint: {}'.format(name))\n",
    "    model.load_state_dict(torch.load(name, map_location='cpu'))\n",
    "    epoch = int(os.path.splitext(os.path.basename(name))[0].split('-')[1])\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def net_output_to_predictions(output):\n",
    "    _, predicted_colors = output['color'].cpu().max(1)\n",
    "    _, predicted_genders = output['gender'].cpu().max(1)\n",
    "    _, predicted_articles = output['article'].cpu().max(1)\n",
    "\n",
    "    return predicted_colors.numpy().tolist(), predicted_genders.numpy().tolist(), predicted_articles.numpy().tolist()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device, logger=None, epoch=None, checkpoint=None):\n",
    "    if checkpoint is not None:\n",
    "        checkpoint_load(model, checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    color_predictions = []\n",
    "    gender_predictions = []\n",
    "    article_predictions = []\n",
    "    with torch.no_grad():\n",
    "        avg_loss = 0\n",
    "        accuracy_color = 0\n",
    "        accuracy_gender = 0\n",
    "        accuracy_article = 0\n",
    "\n",
    "        for batch in dataloader:\n",
    "            img = batch['img']\n",
    "            target_labels = batch['labels']\n",
    "            target_labels = {t: target_labels[t].to(device) for t in target_labels}\n",
    "            output = model(img.to(device))\n",
    "\n",
    "            val_train, val_train_losses = model.get_loss(output, target_labels)\n",
    "            avg_loss += val_train.item()\n",
    "            batch_accuracy_color, batch_accuracy_gender, batch_accuracy_article = \\\n",
    "                calculate_metrics(output, target_labels)\n",
    "\n",
    "            accuracy_color += batch_accuracy_color\n",
    "            accuracy_gender += batch_accuracy_gender\n",
    "            accuracy_article += batch_accuracy_article\n",
    "\n",
    "            (batch_color_predictions,\n",
    "             batch_gender_predictions,\n",
    "             batch_article_predictions) = net_output_to_predictions(output)\n",
    "\n",
    "            color_predictions.extend(batch_color_predictions)\n",
    "            gender_predictions.extend(batch_gender_predictions)\n",
    "            article_predictions.extend(batch_article_predictions)\n",
    "\n",
    "    n_samples = len(dataloader)\n",
    "    avg_loss /= n_samples\n",
    "    accuracy_color /= n_samples\n",
    "    accuracy_gender /= n_samples\n",
    "    accuracy_article /= n_samples\n",
    "    print('-' * 72)\n",
    "    print(\"Validation  loss: {:.4f}, color: {:.4f}, gender: {:.4f}, article: {:.4f}\\n\".format(\n",
    "        avg_loss, accuracy_color, accuracy_gender, accuracy_article))\n",
    "\n",
    "    if logger is not None and epoch is not None:\n",
    "        logger.add_scalar(\"val_loss\", avg_loss, epoch)\n",
    "        logger.add_scalar(\"val_accuracy/color\", accuracy_color, epoch)\n",
    "        logger.add_scalar(\"val_accuracy/gender\", accuracy_gender, epoch)\n",
    "        logger.add_scalar(\"val_accuracy/article\", accuracy_article, epoch)\n",
    "    model.train()\n",
    "\n",
    "    return color_predictions, gender_predictions, article_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gh5zSmYaoVB"
   },
   "source": [
    "Now, we're ready to define the training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ah8-cwuxchHN"
   },
   "outputs": [],
   "source": [
    "# from train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def get_cur_time():\n",
    "    return datetime.strftime(datetime.now(), '%Y-%m-%d_%H-%M')\n",
    "\n",
    "\n",
    "def checkpoint_save(model, name, epoch):\n",
    "    f = os.path.join(name, 'checkpoint-{:06d}.pth'.format(epoch))\n",
    "    torch.save(model.state_dict(), f)\n",
    "    print('Saved checkpoint:', f)\n",
    "\n",
    "    return f\n",
    "    \n",
    "\n",
    "\n",
    "def train(start_epoch=1, N_epochs=50, batch_size=16, num_workers=8):\n",
    "    attributes_file = 'fashion-product-images/styles.csv'\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    # attributes variable contains labels for the categories in the dataset and mapping between string names and IDs\n",
    "    attributes = AttributesDataset(attributes_file)\n",
    "\n",
    "    # specify image transforms for augmentation during training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0),\n",
    "        transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.8, 1.2),\n",
    "                                shear=None, resample=False, fillcolor=(255, 255, 255)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    # during validation we use only tensor and normalization transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    train_dataset = FashionDataset('fashion-product-images/train.csv', attributes, train_transform)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    val_dataset = FashionDataset('fashion-product-images/val.csv', attributes, val_transform)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    model = MultiOutputModel(n_color_classes=attributes.num_colors,\n",
    "                             n_gender_classes=attributes.num_genders,\n",
    "                             n_article_classes=attributes.num_articles)\\\n",
    "                            .to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    logdir = os.path.join('logs', get_cur_time())\n",
    "    print(logdir)\n",
    "    savedir = os.path.join('checkpoints', get_cur_time())\n",
    "    print(savedir)\n",
    "    os.makedirs(logdir, exist_ok=False)\n",
    "    os.makedirs(savedir, exist_ok=False)\n",
    "    logger = SummaryWriter(logdir)\n",
    "\n",
    "    n_train_samples = len(train_dataloader)\n",
    "\n",
    "    \n",
    "    visualize_gt_data(val_dataset, attributes)\n",
    "    print(\"\\nAll gender labels:\\n\", attributes.gender_labels)\n",
    "    print(\"\\nAll color labels:\\n\", attributes.color_labels)\n",
    "    print(\"\\nAll article labels:\\n\", attributes.article_labels)\n",
    "\n",
    "    print(\"Starting training ...\")\n",
    "\n",
    "    checkpoint_path = None\n",
    "    for epoch in range(start_epoch, N_epochs + 1):\n",
    "        total_loss = 0\n",
    "        accuracy_color = 0\n",
    "        accuracy_gender = 0\n",
    "        accuracy_article = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            img = batch['img']\n",
    "            target_labels = batch['labels']\n",
    "            target_labels = {t: target_labels[t].to(device) for t in target_labels}\n",
    "            output = model(img.to(device))\n",
    "\n",
    "            loss_train, losses_train = model.get_loss(output, target_labels)\n",
    "            total_loss += loss_train.item()\n",
    "            batch_accuracy_color, batch_accuracy_gender, batch_accuracy_article = \\\n",
    "                calculate_metrics(output, target_labels)\n",
    "\n",
    "            accuracy_color += batch_accuracy_color\n",
    "            accuracy_gender += batch_accuracy_gender\n",
    "            accuracy_article += batch_accuracy_article\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch {:4d}, loss: {:.4f}, color: {:.4f}, gender: {:.4f}, article: {:.4f}\".format(\n",
    "            epoch,\n",
    "            total_loss / n_train_samples,\n",
    "            accuracy_color / n_train_samples,\n",
    "            accuracy_gender / n_train_samples,\n",
    "            accuracy_article / n_train_samples))\n",
    "\n",
    "        logger.add_scalar('train_loss', total_loss / n_train_samples, epoch)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            checkpoint_path = checkpoint_save(model, savedir, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            validate(model, val_dataloader, device, logger, epoch)\n",
    "            \n",
    "    checkpoint_path = checkpoint_save(model, savedir, epoch - 1)\n",
    "\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TclnDLwbdpbs",
    "outputId": "34caec36-0d24-4b92-bdad-c84a3e366b55"
   },
   "outputs": [],
   "source": [
    "last_checkpoint_path = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlosXO7Fa-px"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "As we have trained models now, let's evaluate them at our validation set. Also we can visualize some additional information, like confusion matrics, that helps us understand our models better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnbdqTcCR32g"
   },
   "outputs": [],
   "source": [
    "# from test.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def visualize_grid(dataset, attributes, color_predictions, gender_predictions, article_predictions):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    predicted_color_all = []\n",
    "    predicted_gender_all = []\n",
    "    predicted_article_all = []\n",
    "\n",
    "    gt_labels = []\n",
    "    gt_color_all = []\n",
    "    gt_gender_all = []\n",
    "    gt_article_all = []\n",
    "\n",
    "    \n",
    "    # store the original transforms from the dataset\n",
    "    transforms = dataset.transform\n",
    "    # and not use them during visualization\n",
    "    dataset.transform = None\n",
    "\n",
    "    for (sample, \n",
    "         predicted_color, \n",
    "         predicted_gender, \n",
    "         predicted_article) in zip(\n",
    "             dataset, color_predictions, gender_predictions, article_predictions):\n",
    "        predicted_color = attributes.color_id_to_name[predicted_color]\n",
    "        predicted_gender = attributes.gender_id_to_name[predicted_gender]\n",
    "        predicted_article = attributes.article_id_to_name[predicted_article]\n",
    "\n",
    "        gt_color = attributes.color_id_to_name[sample['labels']['color_labels']]\n",
    "        gt_gender = attributes.gender_id_to_name[sample['labels']['gender_labels']]\n",
    "        gt_article = attributes.article_id_to_name[sample['labels']['article_labels']]\n",
    "\n",
    "        predicted_color_all.append(predicted_color)\n",
    "        predicted_gender_all.append(predicted_gender)\n",
    "        predicted_article_all.append(predicted_article)\n",
    "\n",
    "        gt_color_all.append(gt_color)\n",
    "        gt_gender_all.append(gt_gender)\n",
    "        gt_article_all.append(gt_article)\n",
    "\n",
    "        imgs.append(sample['img'])\n",
    "        labels.append(\"{}\\n{}\\n{}\".format(predicted_gender, predicted_article, predicted_color))\n",
    "        gt_labels.append(\"{}\\n{}\\n{}\".format(gt_gender, gt_article, gt_color))\n",
    "\n",
    "    # restore original transforms\n",
    "    dataset.transform = transforms\n",
    "\n",
    "    # Draw confusion matrices\n",
    "    # color\n",
    "    cn_matrix = confusion_matrix(\n",
    "        y_true=gt_color_all,\n",
    "        y_pred=predicted_color_all,\n",
    "        labels=attributes.color_labels,\n",
    "        normalize='true')\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 5})\n",
    "    plt.rcParams.update({'figure.dpi': 300})\n",
    "    ConfusionMatrixDisplay(cn_matrix, attributes.color_labels).plot(\n",
    "        include_values=False, xticks_rotation='vertical')\n",
    "    plt.title(\"Colors\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # gender\n",
    "    cn_matrix = confusion_matrix(\n",
    "        y_true=gt_gender_all,\n",
    "        y_pred=predicted_gender_all,\n",
    "        labels=attributes.gender_labels,\n",
    "        normalize='true')\n",
    "    ConfusionMatrixDisplay(cn_matrix, attributes.gender_labels).plot(\n",
    "        xticks_rotation='horizontal')\n",
    "    plt.title(\"Genders\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.rcParams.update({'font.size': 2.5})\n",
    "    cn_matrix = confusion_matrix(\n",
    "        y_true=gt_article_all,\n",
    "        y_pred=predicted_article_all,\n",
    "        labels=attributes.article_labels,\n",
    "        normalize='true')\n",
    "    ConfusionMatrixDisplay(cn_matrix, attributes.article_labels).plot(\n",
    "        include_values=False, xticks_rotation='vertical')\n",
    "    plt.title(\"Article types\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.rcParams.update({'font.size': 5})\n",
    "    plt.rcParams.update({'figure.dpi': 100})\n",
    "    title = \"Predicted labels\"\n",
    "    n_cols = 5\n",
    "    n_rows = 3\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    axs = axs.flatten()\n",
    "    for img, ax, label in zip(imgs, axs, labels):\n",
    "        ax.set_xlabel(label, rotation=0)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        ax.imshow(img)\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def test(checkpoint_path):\n",
    "    attributes_file = 'fashion-product-images/styles.csv'\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    # attributes variable contains labels for the categories in the dataset and mapping between string names and IDs\n",
    "    attributes = AttributesDataset(attributes_file)\n",
    "\n",
    "    # during validation we use only tensor and normalization transforms\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "    test_dataset = FashionDataset('fashion-product-images/val.csv', attributes, val_transform)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)\n",
    "\n",
    "    model = MultiOutputModel(n_color_classes=attributes.num_colors, n_gender_classes=attributes.num_genders,\n",
    "                             n_article_classes=attributes.num_articles).to(device)\n",
    "\n",
    "    model_predictions = validate(model, test_dataloader, device, checkpoint=checkpoint_path)\n",
    "\n",
    "    # Visualization of the trained model\n",
    "    visualize_grid(test_dataset, attributes, *model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hWK2ssBOXofC",
    "outputId": "a09d473e-b2eb-43dd-a288-0e0ba9f29cdf"
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "test(last_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMHOWKlPhowNyOabIEBRoaz",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Multi-Output-Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
