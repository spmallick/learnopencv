{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d01077",
   "metadata": {},
   "source": [
    "<img src = \"https://learnopencv.com/wp-content/uploads/2024/09/Feature-Multimodal-RAG-with-ColPali-Gemini.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e99bdba",
   "metadata": {},
   "source": [
    "#### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee3fb7-9d95-45be-9a5f-1a9e139ac004",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdf2image einops google-generativeai gradio -q\n",
    "!pip install colpali-engine==0.2.2 -q\n",
    "!pip install -U bitsandbytes -q\n",
    "!pip install mteb transformers tqdm typer seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bd4f7-3867-4123-b4bd-129a83411c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in terminals\n",
    "sudo apt install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "444067e1-a756-4160-855b-cbfd75c7d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "from colpali_engine.models.paligemma_colbert_architecture import ColPali\n",
    "from colpali_engine.trainer.retrieval_evaluator import CustomEvaluator\n",
    "from colpali_engine.utils.colpali_processing_utils import process_images, process_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c589f8c",
   "metadata": {},
   "source": [
    "#### Load Model from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "452aeda7-865c-43af-9e92-d99912bd2bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"vidore/colpali\"\n",
    "hf_token = getpass.getpass(\"Enter HF API: \")\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "model = ColPali.from_pretrained(\n",
    "    \"google/paligemma-3b-mix-448\", torch_dtype=torch.bfloat16, device_map=\"cuda\", token=hf_token\n",
    ").eval()\n",
    "model.load_adapter(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name, token=hf_token)\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ab980",
   "metadata": {},
   "source": [
    "### ColPali"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d4e32",
   "metadata": {},
   "source": [
    "1. Offline Indexing - ColPali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defe2323-f9b5-4c03-89a4-09198da1b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(file, ds):\n",
    "    \n",
    "    images = []\n",
    "    for f in file:\n",
    "        images.extend(convert_from_path(f))\n",
    "\n",
    "    # run inference - docs\n",
    "    dataloader = DataLoader(\n",
    "        images,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: process_images(processor, x),\n",
    "    )\n",
    "    for batch_doc in tqdm(dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch_doc = {k: v.to(device) for k, v in batch_doc.items()}\n",
    "            embeddings_doc = model(**batch_doc)\n",
    "        ds.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n",
    "    return f\"Uploaded and converted {len(images)} pages\", ds, images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d39f2",
   "metadata": {},
   "source": [
    "2. Online Querying - ColPali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, ds, images):\n",
    "    qs = []\n",
    "    with torch.no_grad():\n",
    "        batch_query = process_queries(processor, [query], mock_image)\n",
    "        batch_query = {k: v.to(device) for k, v in batch_query.items()}\n",
    "        embeddings_query = model(**batch_query)\n",
    "        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n",
    "\n",
    "    # run evaluation\n",
    "    retriever_evaluator = CustomEvaluator(is_multi_vector=True)\n",
    "    scores = retriever_evaluator.evaluate(qs, ds)\n",
    "    best_page = int(scores.argmax(axis=1).item())\n",
    "    return f\"The most relevant page is {best_page}\", images[best_page]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec43c13",
   "metadata": {},
   "source": [
    "### Google Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ca16a6-8cdd-4271-8c49-8a9a16502008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={'temperature': 0.0, 'top_p': 0.95, 'top_k': 64, 'max_output_tokens': 1024, 'response_mime_type': 'text/plain'},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "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\n",
    "generation_config = {\n",
    "  \"temperature\": 0.0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 1024,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyC-j70oiSBB-Ta9-6ptqMAYcv4aUVNop0w\")\n",
    "\n",
    "gemini_flash = genai.GenerativeModel(model_name=\"gemini-1.5-flash\" , generation_config=generation_config)\n",
    "\n",
    "def get_answer(prompt:str , image:Image):\n",
    "  response = model.generate_content([prompt, image])\n",
    "  return response.text\n",
    "    \n",
    "gemini_flash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7d642",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38cb8c2-f723-4086-86b2-2b4b29c0cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://53354a1cb47d90d401.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://53354a1cb47d90d401.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gradio/route_utils.py\", line 321, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_3873/1516570879.py\", line 4, in index\n",
      "    for f in file:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:13<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50])\n",
      "Top 1 Accuracy (verif): 0.0\n",
      "tensor([23])\n",
      "Top 1 Accuracy (verif): 0.0\n",
      "tensor([23])\n",
      "Top 1 Accuracy (verif): 0.0\n",
      "tensor([55])\n",
      "Top 1 Accuracy (verif): 0.0\n",
      "tensor([55])\n",
      "Top 1 Accuracy (verif): 0.0\n",
      "tensor([25])\n",
      "Top 1 Accuracy (verif): 0.0\n"
     ]
    }
   ],
   "source": [
    "COLORS = [\"#4285f4\", \"#db4437\", \"#f4b400\", \"#0f9d58\", \"#e48ef1\"]\n",
    "\n",
    "mock_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ColPali: Efficient Document Retrieval with Vision Language Models ðŸ“šðŸ”\")\n",
    "    gr.Markdown(\"## 1ï¸âƒ£ Upload PDFs\")\n",
    "    file = gr.File(file_types=[\"pdf\"], file_count=\"multiple\")\n",
    "\n",
    "    gr.Markdown(\"## 2ï¸âƒ£ Index the PDFs and upload\")\n",
    "    convert_button = gr.Button(\"ðŸ”„ Convert and upload\")\n",
    "    message = gr.Textbox(\"Files not yet uploaded\")\n",
    "    embeds = gr.State(value=[])\n",
    "    imgs = gr.State(value=[])\n",
    "\n",
    "    # Define the actions for conversion\n",
    "    convert_button.click(index, inputs=[file, embeds], outputs=[message, embeds, imgs])\n",
    "\n",
    "    gr.Markdown(\"## 3ï¸âƒ£ Search\")\n",
    "    query = gr.Textbox(placeholder=\"Enter your query to match\")\n",
    "    search_button = gr.Button(\"ðŸ” Search\")\n",
    "\n",
    "    gr.Markdown(\"## 4ï¸âƒ£ ColPali Retrieval\")\n",
    "    message2 = gr.Textbox(\"Most relevant image is...\")\n",
    "    output_img = gr.Image()\n",
    "\n",
    "    gr.Markdown(\"## 5ï¸âƒ£ Gemini Response\")\n",
    "    output_text = gr.Textbox(\"Gemini Response...\")\n",
    "\n",
    "    def get_answer(prompt:str , image:Image):\n",
    "       response = gemini_flash.generate_content([prompt, image])\n",
    "       return response.text\n",
    "\n",
    "    # Function to combine retrieval and LLM call\n",
    "    def search_with_llm(query, ds, images, prompt=\"What is shown in this image, analyse and provide some interpretation? Format the answer in a neat 500 words summary.\"):\n",
    "        # Step 1: Search the best image based on query\n",
    "        search_message, best_image = search(query, ds, images)\n",
    "\n",
    "        # Step 2: Generate an answer using LLM\n",
    "        answer = get_answer(prompt, best_image)\n",
    "\n",
    "        return search_message, best_image, answer\n",
    "\n",
    "    # Action for search button\n",
    "    search_button.click(\n",
    "        search_with_llm,\n",
    "        inputs=[query, embeds, imgs],\n",
    "        outputs=[message2, output_img, output_text]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue(max_size=10).launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42cfd1-a7e9-4f91-92ef-e93b1cbb6335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
