{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center> <font style=\"color:rgb(100,109,254)\"> Playing Chrome's T-Rex Game with Augmented Reality </font> </center>**\n",
    "\n",
    "## Outline:\n",
    "\n",
    "- **Step 1: Real-time Face Detection**\n",
    "- **Step 2: Find the landmarks for the detected face**\n",
    "- **Step 3: Build the Jump Control mechanism for the Dino**\n",
    "- **Step 4: Build the Crouch Control mechanism**\n",
    "- **Step 5: Perform Calibration**\n",
    "- **Step 6: Keyboard Automation with PyautoGUI**\n",
    "- **Step 7: Build the Final Application**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import hypot\n",
    "import pyautogui\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize The DNN Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the weights file\n",
    "model_weights =  \"model/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "\n",
    "# Path to the architecture file\n",
    "model_arch = \"model/deploy.prototxt.txt\"\n",
    "\n",
    "# Load the caffe model\n",
    "net = cv2.dnn.readNetFromCaffe(model_arch, model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Face Detection Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detector(image, threshold =0.7):\n",
    "    \n",
    "    # Get the height,width of the image\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Apply mean subtraction, and create 4D blob from image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0,(300, 300), (104.0, 117.0, 123.0))\n",
    "    \n",
    "    # Set the new input value for the network\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    # Run forward pass on the input to compute output\n",
    "    faces = net.forward()\n",
    "    \n",
    "    # Get the confidence value for all detected faces\n",
    "    prediction_scores = faces[:,:,:,2]\n",
    "    \n",
    "    # Get the index of the prediction with highest confidence \n",
    "    i = np.argmax(prediction_scores)\n",
    "    \n",
    "    # Get the face with highest confidence \n",
    "    face = faces[0,0,i]\n",
    "    \n",
    "    # Extract the confidence\n",
    "    confidence = face[2]\n",
    "    \n",
    "    # if confidence value is greater than the threshold\n",
    "    if confidence > threshold:\n",
    "        \n",
    "        # The 4 values at indexes 3 to 6 are the top-left, bottom-right coordinates\n",
    "        # scales to range 0-1.The original coordinates can be found by \n",
    "        # multiplying x,y values with the width,height of the image\n",
    "        box = face[3:7] * np.array([w, h, w, h])\n",
    "        \n",
    "        # The coordinates are the pixel numbers relative to the top left\n",
    "        # corner of the image therefore needs be quantized to int type\n",
    "        (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "        \n",
    "        # Draw a bounding box around the face.\n",
    "        annotated_frame = cv2.rectangle(image.copy(), (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        output = (annotated_frame, (x1, y1, x2, y2), True, confidence)\n",
    "    \n",
    "    # Return the original frame if no face is detected with high confidence.\n",
    "    else:\n",
    "        output = (image,(),False, 0)\n",
    "    \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the **`face_detector()`** function with a real-time camera feed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('face Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    annotated_frame, coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('face Detection', annotated_frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Landmarks Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Up the facial landmark Detector:\n",
    "\n",
    "Download [detection model from here](https://github.com/italojs/facial-landmarks-recognition/blob/master/shape_predictor_68_face_landmarks.dat) and put it inside the `model` folder inside this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the landmark detector\n",
    "predictor = dlib.shape_predictor(\"model/shape_predictor_68_face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the detect_landmarks() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmarks(box, image):\n",
    "    \n",
    "    # For faster results convert the image to gray-scale\n",
    "    gray_scale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Get the coordinates \n",
    "    (x1, y1, x2, y2) = box\n",
    "\n",
    "    # Perform the detection\n",
    "    shape = predictor(gray_scale, dlib.rectangle(x1, y1, x2, y2))\n",
    "    \n",
    "    # Get the numPy array containing the coordinates of the landmarks\n",
    "    landmarks = shape_to_np(shape)\n",
    "    \n",
    "   # Draw the landmark points with circles \n",
    "    for (x, y) in landmarks:\n",
    "        annotated_image = cv2.circle(image, (x, y),2, (0, 127, 255), -1)\n",
    "\n",
    "    return annotated_image, landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the **`shape_to_np()`** helper function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape): \n",
    "    \n",
    "    # Create an array of shape (68, 2) for storing the landmark coordinates\n",
    "    landmarks = np.zeros((68, 2), dtype=\"int\")\n",
    "    \n",
    "    # Write the x,y coordinates of each landmark into the array\n",
    "    for i in range(0, 68): \n",
    "        landmarks[i] = (shape.part(i).x, shape.part(i).y)\n",
    "        \n",
    "        \n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test **`detect_landmarks()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Landmark Detection', cv2.WINDOW_NORMAL) \n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Landmark Detection',landmark_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Jump Control mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mouth_open(landmarks, ar_threshold = 0.7): \n",
    "    \n",
    "    \n",
    "    # Calculate the euclidean distance labelled as A,B,C\n",
    "    A = hypot(landmarks[50][0] - landmarks[58][0], landmarks[50][1] - landmarks[58][1])\n",
    "    B = hypot(landmarks[52][0] - landmarks[56][0], landmarks[52][1] - landmarks[56][1])\n",
    "    C = hypot(landmarks[48][0] - landmarks[54][0], landmarks[48][1] - landmarks[54][1])\n",
    "    \n",
    "    # Calculate the mouth aspect ratio\n",
    "    # The value of vertical distance A,B is averaged\n",
    "    mouth_aspect_ratio = (A + B) / (2.0 * C)\n",
    "    \n",
    "    # Return True if the value is greater than the threshold\n",
    "    if mouth_aspect_ratio > ar_threshold:\n",
    "        return True, mouth_aspect_ratio\n",
    "    else:\n",
    "        return False, mouth_aspect_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the **`is_mouth_open()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Mouth Status', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Get the landmarks for the face region in the frame\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Adjust the threshold and make sure it's working for you.\n",
    "        mouth_status,_ = is_mouth_open(landmarks, ar_threshold = 0.6)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(frame,'Is Mouth Open: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Mouth Status',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Crouch Control Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_proximity(box,image, proximity_threshold = 325):\n",
    "    \n",
    "    # Get the height and width of the face bounding box\n",
    "    face_width =  box[2]-box[0]\n",
    "    face_height = box[3]-box[1]\n",
    "    \n",
    "    # Draw rectangle to guide the user \n",
    "    # Calculate the angle of diagonal using face width, height \n",
    "    theta = np.arctan(face_height/face_width)\n",
    "     \n",
    "    # Use the angle to calculate height, width of the guide rectangle\n",
    "    guide_height = np.sin(theta)*proximity_threshold\n",
    "    guide_width  = np.cos(theta)*proximity_threshold\n",
    "    \n",
    "    # Calculate the mid-point of the guide rectangle/face bounding box\n",
    "    mid_x,mid_y = (box[2]+box[0])/2 , (box[3]+box[1])/2\n",
    "    \n",
    "    #Calculate to coordinates of top-left and bottom-right corners\n",
    "    guide_topleft = int(mid_x-(guide_width/2)), int(mid_y-(guide_height/2))\n",
    "    guide_bottomright = int(mid_x +(guide_width/2)), int(mid_y + (guide_height/2))\n",
    "    \n",
    "    # Draw the guide rectangle\n",
    "    cv2.rectangle(image, guide_topleft, guide_bottomright, (0, 255, 255), 2)\n",
    "    \n",
    "    # Calculate the diagonal distance of the bounding box\n",
    "    diagonal = hypot(face_width, face_height)\n",
    "    \n",
    "    # Return True if distance greater than the threshold\n",
    "    if diagonal > proximity_threshold:\n",
    "        return True, diagonal\n",
    "    else:\n",
    "        return False, diagonal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the **`face_proximity()`** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Face proximity', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Check if face is closer than the defined threshold\n",
    "        is_face_close,_ = face_proximity(box_coords, face_image, proximity_threshold = 325)\n",
    "        \n",
    "        # Display the mouth status\n",
    "        cv2.putText(face_image,'Is Face Close: {}'.format(is_face_close),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face proximity',face_image)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Perform Calibration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Calibration', cv2.WINDOW_NORMAL)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if the frame is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Get the current mouth aspect ratio\n",
    "        _,mouth_ar = is_mouth_open(landmarks)\n",
    "    \n",
    "        # Get the current face proximity\n",
    "        _, proximity  = face_proximity(box_coords, face_image)\n",
    "\n",
    "        # Calculate threshold values\n",
    "        ar_threshold = mouth_ar*1.4\n",
    "        proximity_threshold = proximity*1.3\n",
    "\n",
    "        \n",
    "        # Dsiplay the threshold values\n",
    "        cv2.putText(frame, 'Aspect ratio threshold: {:.2f} '.format(ar_threshold), \n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        cv2.putText(frame,'Proximity threshold: {:.2f}'.format(proximity_threshold), \n",
    "                    (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "     \n",
    "    # Display the frame\n",
    "    cv2.imshow('Calibration',frame)\n",
    "    \n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture and destroy the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Keyboard Automation\n",
    "***Note:*** *When running the following cells in Jupyter Notebook, make sure you don't use the **Shift + Enter** command to run the following code cells. You can use the Run code cell button in the Toolbar.* *Also if the keyboard buttons misbehave then you can also restart the kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will open a context menu\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press space bar. This will scroll down the page in some browsers\n",
    "pyautogui.press('space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To press multiple keys we can pass a list of strings to **`press()`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will move the focus to the next cell in the notebook\n",
    "pyautogui.press(['shift','enter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use **`pyautogui.keyDown()`**  instead of **`pyautogui.press()`**. Then the specified key is held down unless  **`pyautogui.keyUp()`**event takes place helping us trigger a longer key press."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold down the shift key\n",
    "pyautogui.keyDown('shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press enter while the shift key is down, this will run the next code cell\n",
    "pyautogui.press('enter')\n",
    "\n",
    "# Release the shift key\n",
    "pyautogui.keyUp('shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run automatically after running the two code cells above\n",
    "print('I ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7:  Build The Final Application\n",
    "\n",
    "Go to **`Chrome://Dino`** in your Chrome browser and run the code cell below.\n",
    "\n",
    "***Note:*** *The image window screen will freeze when you trigger key buttons since at that moment the while loop will pause to press the key. So don't worry about that, after the program launches minimize the camera window and just go to chrome://dino/ and start playing using your face and mouth.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the video feed from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Set the window to a normal one so we can adjust it\n",
    "cv2.namedWindow('Dino with OpenCV', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# By default each key press is followed by a 0.1 second pause\n",
    "pyautogui.PAUSE = 0.0\n",
    "\n",
    "# The fail-safe triggers an exception in case mouse\n",
    "# is moved to corner of the screen\n",
    "#pyautogui.FAILSAFE = False\n",
    "\n",
    "while(True):\n",
    "    \n",
    "     # Read the frames\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Break if frame is not returned\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    # Flip the frame\n",
    "    frame = cv2.flip( frame, 1 )\n",
    "    \n",
    "    # Detect face in the frame\n",
    "    face_image, box_coords, status, conf = face_detector(frame)\n",
    "    \n",
    "    if status:\n",
    "        \n",
    "        # Detect landmarks if a face is found\n",
    "        landmark_image, landmarks = detect_landmarks(box_coords, frame)\n",
    "        \n",
    "        # Check if mouth is open\n",
    "        is_open,_ = is_mouth_open(landmarks, ar_threshold)\n",
    "        \n",
    "        # If the mouth is open trigger space key Down event to jump\n",
    "        if is_open:\n",
    "            \n",
    "            pyautogui.keyDown('space')\n",
    "            mouth_status = 'Open'\n",
    "\n",
    "        else:\n",
    "            # Else the space key is Up\n",
    "            pyautogui.keyUp('space')\n",
    "            mouth_status = 'Closed'\n",
    "        \n",
    "        # Display the mouth status on the frame\n",
    "        cv2.putText(frame,'Mouth: {}'.format(mouth_status),\n",
    "                    (20, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 127, 255),2)\n",
    "        \n",
    "        # Check the proximity of the face\n",
    "        is_closer,_  = face_proximity(box_coords, frame, proximity_threshold)\n",
    "        \n",
    "        # If face is closer press the down key\n",
    "        if is_closer:\n",
    "            pyautogui.keyDown('down')\n",
    "            \n",
    "        else:\n",
    "            pyautogui.keyUp('down')\n",
    "        \n",
    "    # Display the frame\n",
    "    cv2.imshow('Dino with OpenCV',frame)\n",
    "\n",
    "    # Break the loop if 'q' key pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
