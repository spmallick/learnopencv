{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanyam83/learnopencv/blob/master/ALPR/ALPR_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtgYzfAaJMCb"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jFrfraVDVIe"
      },
      "outputs": [],
      "source": [
        "# Cloning DeepSORT\n",
        "!git clone https://github.com/nwojke/deep_sort.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original DeepSORT repo uses a deprecated sklearn function called `linear_assignment`, which needs to be replaced for error free execution of code with scipy.\n",
        "\n",
        "\n",
        "\n",
        "1.   Open ./deep_sort/deep_sort/linear_assignment.py\n",
        "2.   Replace `from sklearn.utils.linear_assignment_ import linear_assignment` in line 4 with `from scipy.optimize import linear_sum_assignment`.\n",
        "\n",
        "3.   Replace `indices = linear_assignment(cost_matrix)` in line 58 with the following lines of code:\n",
        "```\n",
        "  indices = linear_sum_assignment(cost_matrix)\n",
        "  indices = np.asarray(indices)\n",
        "  indices = np.transpose(indices)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JJ_xmWa3mUsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, rename ./deep_sort/tools as ./deep_sort/tools_deepsort to avoid any name overlapping."
      ],
      "metadata": {
        "id": "3JtIEFrCoyNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSORT imports.\n",
        "%cd ./deep_sort\n",
        "from application_util import preprocessing\n",
        "from deep_sort import nn_matching\n",
        "from deep_sort.detection import Detection\n",
        "from deep_sort.tracker import Tracker\n",
        "from tools_deepsort import generate_detections as gdet\n",
        "import uuid"
      ],
      "metadata": {
        "id": "Kitn5vISMnFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-NNkCyMLDIh"
      },
      "source": [
        "## OCR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpMIk5cmlZt9"
      },
      "source": [
        "##Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "id": "_sFAdI85qDFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No need to install if already installed requirements.txt\n",
        "!pip install paddlepaddle-gpu\n",
        "!pip install \"paddleocr>=2.0.1\""
      ],
      "metadata": {
        "id": "uTiYwfZIUB0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHbxzrx4ljde"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2XTYkQ0Hejw"
      },
      "outputs": [],
      "source": [
        "from paddleocr import PaddleOCR\n",
        "ocr = PaddleOCR(lang='en',rec_algorithm='CRNN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4dcCivKR1g6"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48sYDjF5K7t6"
      },
      "source": [
        "## Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuing process from License plate detection [notebook](https://colab.research.google.com/github/sanyam83/learnopencv/blob/master/ALPR/License_plate_detection_YOLOv4.ipynb). (Assuming the files and weights are now created)"
      ],
      "metadata": {
        "id": "rnl2k6MGtr72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daJDYQBxs88n"
      },
      "outputs": [],
      "source": [
        "# Importing libraries and required functionalities.\n",
        "%cd ./darknet\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import darknet\n",
        "import subprocess\n",
        "import uuid\n",
        "\n",
        "import sys\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Darknet object detector imports.\n",
        "from darknet_images import load_images\n",
        "from darknet_images import image_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x8MAqnHyx1kB"
      },
      "outputs": [],
      "source": [
        "%cd ../\n",
        "# Add absolute paths according to your folder strcuture.\n",
        "# Declaring important variables.\n",
        "# Path of Configuration file of YOLOv4.\n",
        "config_file = '/content/gdrive/MyDrive/yolov4-darknet/darknet/cfg/yolov4-obj.cfg'\n",
        "# Path of obj.data file.\n",
        "data_file = '/content/gdrive/MyDrive/yolov4-darknet/darknet/data/obj.data'\n",
        "# Batch size of data passed to the detector.\n",
        "batch_size = 1\n",
        "# Path to trained YOLOv4 weights.\n",
        "weights = '/content/gdrive/MyDrive/yolov4-darknet/checkpoint/yolov4-obj_best.weights'\n",
        "# Confidence threshold.\n",
        "thresh = 0.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables storing colors and fonts.\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "blue_color = (255,0,0)\n",
        "white_color = (255,255,255)\n",
        "black_color = (0,0,0)\n",
        "green_color = (0,255,0)\n",
        "yellow_color = (178, 247, 218)"
      ],
      "metadata": {
        "id": "gS1-J_w7WG_2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zaUUuVXR-7E"
      },
      "source": [
        "\n",
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crop(image, coord):\n",
        "  # Cropping is done by -> image[y1:y2, x1:x2].\n",
        "  cr_img = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
        "  return cr_img"
      ],
      "metadata": {
        "id": "Se2fpL4PWFDG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_bbox(detections, out_size, in_size):\n",
        "  coord = []\n",
        "  scores = []\n",
        "\n",
        "  # Scaling the bounding boxes to the different size\n",
        "  for det in detections:\n",
        "    points = list(det[2])\n",
        "    conf = det[1]\n",
        "    xmin, ymin, xmax, ymax = darknet.bbox2points(points)\n",
        "    y_scale = float(out_size[0]) / in_size[0]\n",
        "    x_scale = float(out_size[1]) / in_size[1]\n",
        "    ymin = int(y_scale * ymin)\n",
        "    ymax = int(y_scale * ymax)\n",
        "    xmin = int(x_scale * xmin) if int(x_scale * xmin) > 0 else 0\n",
        "    xmax = int(x_scale * xmax)\n",
        "    final_points = [xmin, ymin, xmax-xmin, ymax-ymin]\n",
        "    scores.append(conf)\n",
        "    coord.append(final_points)\n",
        "  return coord, scores"
      ],
      "metadata": {
        "id": "cuIBZK8Klffb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OYt8qcGRV7PU"
      },
      "outputs": [],
      "source": [
        "def yolo_det(frame, config_file, data_file, batch_size, weights, threshold, output, network, class_names, class_colors, save = False, out_path = ''):\n",
        "\n",
        "  prev_time = time.time()\n",
        "  \n",
        "  # Preprocessing the input image.\n",
        "  width = darknet.network_width(network)\n",
        "  height = darknet.network_height(network)\n",
        "  darknet_image = darknet.make_image(width, height, 3)\n",
        "  image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "  image_resized = cv2.resize(image_rgb, (width, height))\n",
        "  \n",
        "  # Passing the image to the detector and store the detections\n",
        "  darknet.copy_image_from_bytes(darknet_image, image_resized.tobytes())\n",
        "  detections = darknet.detect_image(network, class_names, darknet_image, thresh=threshold)\n",
        "  darknet.free_image(darknet_image)\n",
        "\n",
        "  # Plotting the deetections using darknet in-built functions\n",
        "  image = darknet.draw_boxes(detections, image_resized, class_colors)\n",
        "  print(detections)\n",
        "  if save:\n",
        "    im = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    file_name = out_path + str(uuid.uuid4()) +'-det.jpg'\n",
        "    cv2.imwrite(os.path.join(output, file_name), im)\n",
        "\n",
        "  # Calculating time taken and FPS for detection\n",
        "  det_time = time.time() - prev_time\n",
        "  fps = int(1/(time.time() - prev_time))\n",
        "  print(\"Detection time: {}\".format(det_time))\n",
        "  \n",
        "  # Resizing predicted bounding box from 416x416 to input image resolution\n",
        "  out_size = frame.shape[:2]\n",
        "  in_size = image_resized.shape[:2]\n",
        "  if detections:\n",
        "    coord, scores = resize_bbox(detections, out_size, in_size)\n",
        "    return coord, scores, det_time\n",
        "  else:\n",
        "    scores = 0\n",
        "    return detections,scores, det_time "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_ocr(preds, rec_conf, ocr_res, track_id):\n",
        "  for info in preds:\n",
        "    # Check if it is current track id\n",
        "    if info['track_id'] == track_id:\n",
        "      # Check if the ocr confidenence is maximum or not\n",
        "      if info['ocr_conf'] < rec_conf:\n",
        "        info['ocr_conf'] = rec_conf\n",
        "        info['ocr_txt'] = ocr_res\n",
        "      else:\n",
        "        rec_conf = info['ocr_conf']\n",
        "        ocr_res = info['ocr_txt']\n",
        "      break\n",
        "  return preds, rec_conf, ocr_res"
      ],
      "metadata": {
        "id": "Rx7H0jy9NBd0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48pYyTxHLVb6"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fucntion for inferencing on images "
      ],
      "metadata": {
        "id": "ScLiRLarwMWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_img(input, config_file, weights, out_path):\n",
        "  # Loading darknet network and classes along with the bbox colors.\n",
        "  network, class_names, class_colors = darknet.load_network(\n",
        "            config_file,\n",
        "            data_file,\n",
        "            weights,\n",
        "            batch_size= batch_size\n",
        "        )\n",
        "  \n",
        "  # Reading the image and performing YOLOv4 detection. \n",
        "  img = cv2.imread(input)\n",
        "  bboxes, scores, det_time = yolo_det(img, config_file, data_file, batch_size, weights, thresh, out_path, network, class_names, class_colors)\n",
        "\n",
        "  # Extracting or cropping the license plate and applying the OCR.\n",
        "  for bbox in bboxes:\n",
        "    bbox = [bbox[0], bbox[1], bbox[2]- bbox[0], bbox[3] - bbox[1]]\n",
        "    cr_img = crop(img, bbox)\n",
        "    result = ocr.ocr(cr_img, cls=False, det=False)\n",
        "    ocr_res = result[0][0]\n",
        "    rec_conf = result[0][1]\n",
        "    print(result)\n",
        "    # Plotting the predictions using OpenCV.\n",
        "    (label_width,label_height), baseline = cv2.getTextSize(ocr_res , font, 2, 3)\n",
        "    top_left = tuple(map(int,[int(bbox[0]),int(bbox[1])-(label_height+baseline)]))\n",
        "    top_right = tuple(map(int,[int(bbox[0])+label_width,int(bbox[1])]))\n",
        "    org = tuple(map(int,[int(bbox[0]),int(bbox[1])-baseline]))\n",
        "\n",
        "    cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), blue_color, 2)\n",
        "    cv2.rectangle(img, top_left, top_right, blue_color,-1)\n",
        "    cv2.putText(img, ocr_res, org, font, 2, white_color,3)\n",
        "\n",
        "  # Writing output image.\n",
        "  file_name = os.path.join(out_path, 'out_' + input.split('/')[-1])\n",
        "  cv2.imwrite(file_name, img)"
      ],
      "metadata": {
        "id": "dw-qsMd9l_gd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_img('/content/72b88b2f-33b4-49e6-84ba-32b7df5a181c.jpg', config_file, weights, '/content/')"
      ],
      "metadata": {
        "id": "srBil8r7_Nlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for inferencing on videos"
      ],
      "metadata": {
        "id": "KgcAjQ8QwGWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_vid(vid_dir, config_file, weights,out_path):\n",
        "  # Declaring variables for video processing.\n",
        "  cap = cv2.VideoCapture(vid_dir)\n",
        "  codec = cv2.VideoWriter_fourcc(*'XVID')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  file_name = os.path.join(out_path, 'out_' + vid_dir.split('/')[-1])\n",
        "  out = cv2.VideoWriter(file_name, codec, fps, (width, height))\n",
        "  \n",
        "  # Frame count variable.\n",
        "  ct = 0\n",
        "  \n",
        "  # Loading darknet network and classes along with the bbox colors.\n",
        "  network, class_names, class_colors = darknet.load_network(\n",
        "          config_file,\n",
        "          data_file,\n",
        "          weights,\n",
        "          batch_size= batch_size\n",
        "      )\n",
        "  \n",
        "  # Reading video frame by frame.\n",
        "  while(cap.isOpened()):\n",
        "    ret, img = cap.read()\n",
        "    if ret == True:\n",
        "        print(ct)\n",
        "\n",
        "        # Noting time for calculating FPS.\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Performing the YOLOv4 detection.\n",
        "        bboxes, scores, det_time = yolo_det(img, config_file, data_file, batch_size, weights, thresh, out_path, network, class_names, class_colors)\n",
        "        \n",
        "        # Extracting or cropping the license plate and applying the OCR.\n",
        "        if list(bboxes):\n",
        "          for bbox in bboxes:\n",
        "            bbox = [bbox[0], bbox[1], bbox[2]- bbox[0], bbox[3] - bbox[1]]\n",
        "            cr_img = crop(img, bbox)\n",
        "            \n",
        "            result = ocr.ocr(cr_img, cls=False, det=False)\n",
        "\n",
        "            print(result)\n",
        "            ocr_res = result[0][0]\n",
        "            rec_conf = result[0][1]\n",
        "\n",
        "            # Plotting the predictions using OpenCV.\n",
        "            txt = ocr_res\n",
        "            (label_width,label_height), baseline = cv2.getTextSize(ocr_res , font,2,3)\n",
        "            top_left = tuple(map(int,[int(bbox[0]),int(bbox[1])-(label_height+baseline)]))\n",
        "            top_right = tuple(map(int,[int(bbox[0])+label_width,int(bbox[1])]))\n",
        "            org = tuple(map(int,[int(bbox[0]),int(bbox[1])-baseline]))\n",
        "\n",
        "            cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), blue_color, 2)\n",
        "            cv2.rectangle(img, top_left, top_right, blue_color, -1)\n",
        "            cv2.putText(img,txt, org, font, 2, white_color, 3)\n",
        "            #cv2.imwrite('/content/{}.jpg'.format(ct), img)\n",
        "\n",
        "          # Calculating time taken and FPS for the whole process.\n",
        "          tot_time = time.time() - prev_time\n",
        "          fps = 1/tot_time\n",
        "          \n",
        "          # Writing information onto the frame and saving it to be processed in a video.\n",
        "          cv2.putText(img, 'frame: %d fps: %s' % (ct, fps),\n",
        "                  (0, int(100 * 1)), cv2.FONT_HERSHEY_PLAIN, 5, (0, 0, 255), thickness=2)\n",
        "          out.write(img)\n",
        "        \n",
        "        ct = ct + 1\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "IEZjmmAX9pDJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dir = 'Pexels Videos 2103099.mp4'\n",
        "out_path = '/content/'\n",
        "test_vid(input_dir, config_file, weights,out_path)"
      ],
      "metadata": {
        "id": "ll_hQhdsrRGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for integrating Tracker"
      ],
      "metadata": {
        "id": "O4ZI0l7Sv-Nq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download pretrained deep association metric model called `mars-small128.pb`, can be downloaded from [here](https://drive.google.com/drive/folders/1n0jB3zwJysi6YDi4n0HVKz5yOZ0eNA2B?usp=sharing) and put under ./model_data/mars-small128.pb"
      ],
      "metadata": {
        "id": "o_apN7fTvNQo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZN0oaVZhXMLg"
      },
      "outputs": [],
      "source": [
        "def tracker_test_vid(vid_dir, config_file, weights,out_path):\n",
        "  # Declaring variables for video processing.\n",
        "  cap = cv2.VideoCapture(vid_dir)\n",
        "  codec = cv2.VideoWriter_fourcc(*'XVID')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "  file_name = os.path.join(out_path, 'out_' + vid_dir.split('/')[-1])\n",
        "\n",
        "  out = cv2.VideoWriter(file_name, codec, fps, (width, height))\n",
        "\n",
        "  # Declaring variables for tracker.\n",
        "  max_cosine_distance = 0.4\n",
        "  nn_budget = None\n",
        "  \n",
        "  # Intializing tracker\n",
        "  model_filename = './model_data/mars-small128.pb'\n",
        "  encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
        "  metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
        "  tracker = Tracker(metric)\n",
        "  \n",
        "  # Initializing some helper variables.\n",
        "  ct = 0\n",
        "  preds = []\n",
        "  total_obj = 0\n",
        "  rec_tot_time = 1\n",
        "  alpha = 0.5\n",
        "  \n",
        "  # Loading darknet network and classes along with the bbox colors.\n",
        "  network, class_names, class_colors = darknet.load_network(\n",
        "          config_file,\n",
        "          data_file,\n",
        "          weights,\n",
        "          batch_size= batch_size\n",
        "      )\n",
        "  \n",
        "  # Reading video frame by frame.\n",
        "  while(cap.isOpened()):\n",
        "    ret, img = cap.read()\n",
        "    if ret == True:\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        print(ct)\n",
        "        \n",
        "        w_scale = w/1.55\n",
        "        h_scale = h/17\n",
        "\n",
        "        # Method to blend two images, here used to make the information box transparent.\n",
        "        overlay_img = img.copy()\n",
        "        cv2.rectangle(img, (int(w_scale), 0), (w, int(h_scale*3.4)), (0,0,0), -1)\n",
        "        cv2.addWeighted(img, alpha, overlay_img, 1 - alpha, 0, overlay_img)\n",
        "\n",
        "        # Noting time for calculating FPS.\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Performing the YOLOv4 detection.\n",
        "        bboxes, scores, det_time = yolo_det(img, config_file, data_file, batch_size, weights, thresh, out_path, network, class_names, class_colors)\n",
        "        \n",
        "        if list(bboxes):\n",
        "          # Getting appearence features of the object.\n",
        "          features = encoder(img, bboxes)\n",
        "          # Storing all the required info in a list.\n",
        "          detections = [Detection(bbox, score, feature) for bbox, score, feature in zip(bboxes, scores, features)]\n",
        "\n",
        "          # Applying tracker.\n",
        "          # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
        "          tracker.predict()\n",
        "          tracker.update(detections)\n",
        "          track_time = time.time() - prev_time\n",
        "          \n",
        "          # Checking if tracks exist.\n",
        "          for track in tracker.tracks:\n",
        "            if not track.is_confirmed() or track.time_since_update > 1:\n",
        "                continue\n",
        "\n",
        "            # Changing track bbox to top left, bottom right coordinates\n",
        "            bbox = list(track.to_tlbr())\n",
        "            \n",
        "            for i in range(len(bbox)):\n",
        "              if bbox[i] < 0:\n",
        "                bbox[i] = 0\n",
        "\n",
        "            # Extracting or cropping the license plate and applying the OCR.\n",
        "            cr_img = crop(img, bbox)\n",
        "            \n",
        "            rec_pre_time = time.time()\n",
        "            result = ocr.ocr(cr_img, cls=False, det=False)\n",
        "            rec_tot_time = time.time() - rec_pre_time\n",
        "\n",
        "            ocr_res = result[0][0]\n",
        "            print(result)\n",
        "            rec_conf = result[0][1]\n",
        "            \n",
        "            if rec_conf == 'nan':\n",
        "              rec_conf = 0\n",
        "\n",
        "            # Storing the ocr output for corresponding track id.\n",
        "            output_frame = {\"track_id\":track.track_id, \"ocr_txt\":ocr_res, \"ocr_conf\":rec_conf}\n",
        "            \n",
        "            # Appending track_id to list only if it does not exist in the list.\n",
        "            if track.track_id not in list(set(ele['track_id'] for ele in preds)):\n",
        "              total_obj = total_obj + 1\n",
        "              preds.append(output_frame)\n",
        "            # Looking for the current track in the list and updating the highest confidence of it.\n",
        "            else:\n",
        "              preds, rec_conf, ocr_res = get_best_ocr(preds, rec_conf, ocr_res, track.track_id)\n",
        "  \n",
        "            # Plotting the predictions using OpenCV.\n",
        "            txt = str(track.track_id) + '. ' + ocr_res\n",
        "            (label_width,label_height), baseline = cv2.getTextSize(txt , font,2,3)\n",
        "            top_left = tuple(map(int,[int(bbox[0]),int(bbox[1])-(label_height+baseline)]))\n",
        "            top_right = tuple(map(int,[int(bbox[0])+label_width,int(bbox[1])]))\n",
        "            org = tuple(map(int,[int(bbox[0]),int(bbox[1])-baseline]))\n",
        "\n",
        "            cv2.rectangle(overlay_img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), blue_color, 2)\n",
        "            cv2.rectangle(overlay_img, top_left, top_right, blue_color, -1)\n",
        "            cv2.putText(overlay_img,txt, org, font, 2, white_color, 3)\n",
        "            #cv2.imwrite('/content/{}.jpg'.format(ct), img)\n",
        "\n",
        "          # Calculating time taken and FPS for the whole process.\n",
        "          tot_time = time.time() - prev_time\n",
        "          fps = 1/tot_time\n",
        "          \n",
        "          # Writing information onto the frame and saving the frame to be processed into a video with title and values of different colors.\n",
        "          if w < 2000:\n",
        "            size = 1\n",
        "          else:\n",
        "            size = 2\n",
        "\n",
        "          # Plotting frame count information on the frame.\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' , font,size,2)\n",
        "          top_left = (int(w_scale) + 10, int(h_scale))\n",
        "          cv2.putText(overlay_img, 'Frame count:', top_left, font, size, green_color, thickness=2)\n",
        "          \n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img,'%d ' % (ct), top_left_r1, font, size, yellow_color, thickness=2)\n",
        "\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' + ' ' + str(ct) , font, size,2)\n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img, 'Total FPS:' , top_left_r1, font, size, green_color, thickness=2)\n",
        "\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Frame count:' + ' ' + str(ct) + 'Total FPS:' , font, size,2)\n",
        "          top_left_r1 = (int(w_scale) + 10 + label_width, int(h_scale))\n",
        "          cv2.putText(overlay_img, '%s' % (int(fps)), top_left_r1, font, size, yellow_color, thickness=2)\n",
        "\n",
        "          # Plotting Total FPS of ANPR information on the frame.\n",
        "          cv2.putText(overlay_img, 'Detection FPS:' ,(top_left[0], int(h_scale*1.7)), font, size, green_color, thickness=2)\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Detection FPS:', font,size,2)\n",
        "          cv2.putText(overlay_img, '%d' % ((int(1/det_time))),(top_left[0] + label_width, int(h_scale*1.7)), font, size, yellow_color, thickness=2)\n",
        "\n",
        "          # Plotting Recognition/OCR FPS of ANPR on the frame.\n",
        "          cv2.putText(overlay_img, 'Recognition FPS:',(top_left[0], int(h_scale*2.42)), font, size, green_color, thickness=2)\n",
        "          (label_width,label_height), baseline = cv2.getTextSize('Recognition FPS:', font,size,2)\n",
        "          cv2.putText(overlay_img, '%s' % ((int(1/rec_tot_time))),(top_left[0] + label_width, int(h_scale*2.42)), font, size, yellow_color, thickness=2)\n",
        "          cv2.imwrite('/content/{}.jpg'.format(ct), overlay_img)\n",
        "          out.write(overlay_img)\n",
        "        \n",
        "        # Increasing frame count.\n",
        "        ct = ct + 1\n",
        "    else:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceaYktcyat_1"
      },
      "source": [
        "**Test Vid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "D5_i7Ncvv13r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "206ce84e-f199-456c-8a1c-f013dc725258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/10yv_NIHNqOqSr7E7W2L29BzNZWVT1kPR/ALPR-ocr\n"
          ]
        }
      ],
      "source": [
        "input_dir = 'Pexels Videos 2103099.mp4'\n",
        "out_path = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB8hq7dFkgsk"
      },
      "outputs": [],
      "source": [
        "tracker_test_vid(input_dir, config_file, weights,out_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qs8tyRNKXK16"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ALPR_inference",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}