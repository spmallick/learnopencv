{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "experiment-logging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
   "cell_type": "markdown",
    "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
  },
 "source": [
  "<a href=\"https://colab.research.google.com/github/spmallick/learnopencv/blob/master/PyTorch-Vision-Experiment-Logging/Torchvision-Experiment-Logging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
 ]
},
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T__5bQVhKp3r",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, we're going to apply experiment tracking to the Object Detection Task on COCO dataset using Torchvision. We are going to use the code from torchvision as is, we won't change anything as the idea is to show how an experiment logging functionality can be brought into some existing training framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhHQFy6P606e",
        "colab_type": "text"
      },
      "source": [
        "# Set everything up\n",
        "\n",
        "In this part, we are going to set up the environment, i.e. download the source\n",
        "code, initialize functions, download and prepare COCO dataset and its annotations, install and import required libraries, and log in to the Weights & Biases tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SaKUS4EaZri",
        "colab_type": "text"
      },
      "source": [
        "## Download torchvision code from sources\n",
        "\n",
        "Using the command below we'll download the detection task code from PyTorch repository. Please, visit [pytorch repository](https://github.com/pytorch/vision) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBvcjvZZapxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/coco_eval.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/coco_utils.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/engine.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/group_by_aspect_ratio.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/train.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/transforms.py && \\\n",
        " wget https://raw.githubusercontent.com/pytorch/vision/3974cfeb447be9f5ef1fc95e5c320f0498fe68ae/references/detection/utils.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZqebrClaIaX",
        "colab_type": "text"
      },
      "source": [
        "## Download COCO dataset\n",
        "In this section, we'll download COCO validation images and annotations. We're going to use the validation set for training and validation.\n",
        "\n",
        "**Please, don't do this in real life as models should be trained on training images and validated on the validation set. Otherwise, you'll face [overfitting](https://https://en.wikipedia.org/wiki/Overfitting)**. \n",
        "\n",
        "So we're using validation set for training because the complete dataset is very large and training on it will take a lot of time. While our goal is to show how the experiment logging works and we want to do it in a reasonable amount of time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3Hbz3IMaugx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir coco && mkdir coco/annotations\n",
        "\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip && \\\n",
        " wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
        "\n",
        "!unzip val2017.zip && unzip annotations_trainval2017.zip\n",
        "\n",
        "!cp -r val2017 coco/train2017\n",
        "!cp annotations/instances_val2017.json coco/annotations/instances_val2017.json\n",
        "\n",
        "!mv val2017 coco/val2017\n",
        "!mv annotations/instances_val2017.json coco/annotations/instances_train2017.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUJAxouGjeAo",
        "colab_type": "text"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6F5KFzSkYDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy==1.17.3\n",
        "!pip install tensorboardX==2.0\n",
        "!pip install wandb==0.8.36"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn-BrTwM41tF",
        "colab_type": "text"
      },
      "source": [
        "## Log in to Weights & Biases\n",
        "\n",
        "Please go to https://www.wandb.com/ and create an account if you don't have one yet. Then, you need to open https://app.wandb.ai/authorize to get the W&B API key to be able to start streaming logging to W&B. Paste an API key from your profile and hit enter after running the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zfNvGTqXtKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUDDmQe06yrt",
        "colab_type": "text"
      },
      "source": [
        "## Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G59dSjd06Xz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import math\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import tensorboardX\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import torchvision.models.detection\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "import wandb\n",
        "\n",
        "import utils\n",
        "from coco_eval import CocoEvaluator\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from engine import _get_iou_types\n",
        "from group_by_aspect_ratio import GroupedBatchSampler, create_aspect_ratio_groups\n",
        "from train import get_transform, get_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-lIOk1E_1gL",
        "colab_type": "text"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVMZRHjfKdFC",
        "colab_type": "text"
      },
      "source": [
        "### COCO labels\n",
        "\n",
        "Let's initialize mapping so as to be able to see human-readable labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvGt0crlJ8sw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# COCO labels mapping\n",
        "COCO_INSTANCE_CATEGORY_NAMES = [\n",
        "    \"__background__\",\"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\n",
        "    \"train\",\"truck\",\"boat\",\"trafficlight\",\"firehydrant\",\"N/A\",\"stopsign\",\n",
        "    \"parkingmeter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\n",
        "    \"bear\",\"zebra\",\"giraffe\",\"N/A\",\"backpack\",\"umbrella\",\"N/A\",\"N/A\",\"handbag\",\n",
        "    \"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sportsball\",\"kite\",\n",
        "    \"baseballbat\",\"baseballglove\",\"skateboard\",\"surfboard\",\"tennisracket\",\n",
        "    \"bottle\",\"N/A\",\"wineglass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\n",
        "    \"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hotdog\",\"pizza\",\"donut\",\n",
        "    \"cake\",\"chair\",\"couch\",\"pottedplant\",\"bed\",\"N/A\",\"diningtable\",\"N/A\",\"N/A\",\n",
        "    \"toilet\",\"N/A\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cellphone\",\n",
        "    \"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"N/A\",\"book\",\"clock\",\n",
        "    \"vase\",\"scissors\",\"teddybear\",\"hairdrier\",\"toothbrush\",\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh3zrzSKKTUE",
        "colab_type": "text"
      },
      "source": [
        "### Define main function\n",
        "\n",
        "Let's take the main function of the code from PyTorch detection as is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htq1VnoxKjHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args):\n",
        "    utils.init_distributed_mode(args)\n",
        "    print(args)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "\n",
        "    # Data loading code\n",
        "    print(\"Loading data\")\n",
        "\n",
        "    dataset, num_classes = get_dataset(args.dataset, \"train\", get_transform(train=True), args.data_path)\n",
        "    dataset_test, _ = get_dataset(args.dataset, \"val\", get_transform(train=False), args.data_path)\n",
        "\n",
        "    print(\"Creating data loaders\")\n",
        "    if args.distributed:\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
        "        test_sampler = torch.utils.data.distributed.DistributedSampler(dataset_test)\n",
        "    else:\n",
        "        train_sampler = torch.utils.data.RandomSampler(dataset)\n",
        "        test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
        "\n",
        "    if args.aspect_ratio_group_factor >= 0:\n",
        "        group_ids = create_aspect_ratio_groups(dataset, k=args.aspect_ratio_group_factor)\n",
        "        train_batch_sampler = GroupedBatchSampler(train_sampler, group_ids, args.batch_size)\n",
        "    else:\n",
        "        train_batch_sampler = torch.utils.data.BatchSampler(\n",
        "            train_sampler, args.batch_size, drop_last=True)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_sampler=train_batch_sampler, num_workers=args.workers,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=5,\n",
        "        sampler=test_sampler, num_workers=args.workers,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    print(\"Creating model\")\n",
        "    model = torchvision.models.detection.__dict__[args.model](num_classes=num_classes,\n",
        "                                                              pretrained=args.pretrained)\n",
        "    model.to(device)\n",
        "\n",
        "    model_without_ddp = model\n",
        "    if args.distributed:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
        "        model_without_ddp = model.module\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params, lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "\n",
        "    # lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.lr_step_size, gamma=args.lr_gamma)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=args.lr_steps, gamma=args.lr_gamma)\n",
        "\n",
        "    if args.resume:\n",
        "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
        "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "        args.start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    if args.test_only:\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "        return\n",
        "\n",
        "    print(\"Start training\")\n",
        "    start_time = time.time()\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        if args.distributed:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, args.print_freq)\n",
        "        lr_scheduler.step()\n",
        "        if args.output_dir:\n",
        "            utils.save_on_master({\n",
        "                'model': model_without_ddp.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'lr_scheduler': lr_scheduler.state_dict(),\n",
        "                'args': args,\n",
        "                'epoch': epoch},\n",
        "                os.path.join(args.output_dir, 'model_{}.pth'.format(epoch)))\n",
        "\n",
        "        # evaluate after every epoch\n",
        "        evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAt7M_0CLZjI",
        "colab_type": "text"
      },
      "source": [
        "# Logging\n",
        "\n",
        "In this part, we'll add TensorBoard and W&B logging to the original PyTorch detection code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auqJJWRJLKV5",
        "colab_type": "text"
      },
      "source": [
        "## Define custom TensorBoard Summary Writer\n",
        "\n",
        "In the next cell, we'll extend the default Summary writer by adding the function that combines plots to subgroups by adding a prefix to a group of loss dictionary keys."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_Wa90RzLHwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a new class inheriting from tensorboardX.SummaryWriter\n",
        "class SummaryWriter(tensorboardX.SummaryWriter):\n",
        "    def __init__(self, log_dir=None, comment=\"\", **kwargs):\n",
        "        super().__init__(log_dir, comment, **kwargs)\n",
        "\n",
        "    # create a new function that will take dictionary as input and uses built-in add_scalar() function\n",
        "    # that function combines all plots into one subgroup by a tag\n",
        "    def add_scalar_dict(self, dictionary, global_step, tag=None):\n",
        "        for name, val in dictionary.items():\n",
        "            if tag is not None:\n",
        "                name = osp.join(tag, name)\n",
        "            self.add_scalar(name, val.item(), global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xDcn0PaLmyx",
        "colab_type": "text"
      },
      "source": [
        "## Applying logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtnU6ZWj6WW5",
        "colab_type": "text"
      },
      "source": [
        "### Tracking loss in the training procedure\n",
        "\n",
        "We want TensorBoard to track the loss. In this section, we'll use our TensorBoard Summary Writer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ5raW_gIUnp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    global global_iter\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        # ### OUR CODE ###\n",
        "        # let's track the losses here by adding scalars\n",
        "        logger.add_scalar_dict(\n",
        "            # passing the dictionary of losses (pairs - loss_key: loss_value)\n",
        "            loss_dict,\n",
        "            # passing the global step (number of iterations)\n",
        "            global_step=global_iter,\n",
        "            # adding the tag to combine plots in a subgroup\n",
        "            tag=\"loss\"\n",
        "        )\n",
        "        # incrementing the global step (number of iterations)\n",
        "        global_iter += 1\n",
        "        # ### END OF OUR CODE ###\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTSJr-n3MEVD",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing predicted bounding boxes in the validation procedure\n",
        "\n",
        "In this part, we'll add the code saves the first 50 images with the predicted bounding boxes and labels for each epoch. That way we will be able to see how they are changing during the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f1cjABEMFRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    # changing these two lines a bit to have iteration number and to keep image tensor\n",
        "    for i, (images, targets) in enumerate(metric_logger.log_every(data_loader, 100, header)):\n",
        "        img = images[0]\n",
        "\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # ### OUR CODE ###\n",
        "        # let's track bounding box and labels predictions for the first 50 images\n",
        "        # as we hardly want to track all validation images\n",
        "        # but want to see how the predicted bounding boxes and labels are changing during the process\n",
        "        if i < 50:\n",
        "            # let's add tracking images with predicted bounding boxes\n",
        "            logger.add_image_with_boxes(\n",
        "                # adding pred_images tag to combine images in one subgroup\n",
        "                \"pred_images/PD-{}\".format(i),\n",
        "                # passing image tensor\n",
        "                img,\n",
        "                # passing predicted bounding boxes\n",
        "                outputs[0][\"boxes\"].cpu(),\n",
        "                # mapping & passing predicted labels\n",
        "                labels=[\n",
        "                    COCO_INSTANCE_CATEGORY_NAMES[i]\n",
        "                    for i in outputs[0][\"labels\"].cpu().numpy()\n",
        "                ],\n",
        "            )\n",
        "        # ### END OUR CODE ###\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OtKa0GSM5tU",
        "colab_type": "text"
      },
      "source": [
        "### Saving config and COCO metrics\n",
        "\n",
        "Finally, we want to track the metric values and bind them to model hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqHEpFp_M4vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CocoEvaluator(CocoEvaluator):\n",
        "    def summarize(self):\n",
        "        global config, total_epochs\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(\"IoU metric: {}\".format(iou_type))\n",
        "            coco_eval.summarize()\n",
        "            # ### OUR CODE ###\n",
        "            if iou_type == \"bbox\":\n",
        "                # let's add hyperparameters and bind them to metric values\n",
        "                logger.add_hparams(\n",
        "                    # passing hyperparameters dictionary (in our case argparse values)\n",
        "                    config,\n",
        "                    # passing COCO metrics\n",
        "                    {\n",
        "                        \"AP/IoU/0.50-0.95/all/100\": coco_eval.stats[0],\n",
        "                        \"AP/IoU/0.50/all/100\": coco_eval.stats[1],\n",
        "                        \"AP/IoU/0.75/all/100\": coco_eval.stats[2],\n",
        "                        \"AP/IoU/0.50-0.95/small/100\": coco_eval.stats[3],\n",
        "                        \"AP/IoU/0.50-0.95/medium/100\": coco_eval.stats[4],\n",
        "                        \"AP/IoU/0.50-0.95/large/100\": coco_eval.stats[5],\n",
        "                        \"AR/IoU/0.50-0.95/all/1\": coco_eval.stats[6],\n",
        "                        \"AR/IoU/0.50-0.95/all/10\": coco_eval.stats[7],\n",
        "                        \"AR/IoU/0.50-0.95/all/100\": coco_eval.stats[8],\n",
        "                        \"AR/IoU/0.50-0.95/small/100\": coco_eval.stats[9],\n",
        "                        \"AR/IoU/0.50-0.95/medium/100\": coco_eval.stats[10],\n",
        "                        \"AR/IoU/0.50-0.95/large/100\": coco_eval.stats[11],\n",
        "                    },\n",
        "                    name=\".\",\n",
        "                    # passing the current iteration (epoch)\n",
        "                    global_step=total_epochs,\n",
        "                )\n",
        "                # incrementing the number of epochs\n",
        "                total_epochs += 1\n",
        "            # ### END OF OUR CODE ###"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5ErQPjgNV7J",
        "colab_type": "text"
      },
      "source": [
        "## Experiment setup\n",
        "\n",
        "Let's create some variables to keep everything organized. Usually, everything is stored in a config file but we use the PyTorch detection code as is and in the project, all hyperparameters are stored using Python argparse. We want to highlight everything that is used for experiment management so in the next cell, we'll create the following variables:\n",
        "\n",
        "\n",
        "\n",
        "*   config - will be used as config i.e. argument dictionary in this project\n",
        "*   log_dir - the place where log files will be saved\n",
        "*   total_epochs - the number of epochs\n",
        "*   global_iter - the number of iterations\n",
        "*   logger - custom TensorBoard SummaryWriter that is used for logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElPLsK8EZ54b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define variables as globals to have an access everywhere\n",
        "config = None  # will be replaced to argparse dictionary\n",
        "total_epochs = 0  # init total number of epochs\n",
        "global_iter = 0  # init total number of iterations\n",
        "\n",
        "name = \"exp-000\"  # init experiment name\n",
        "log_dir = \"experiments\"  # init path where tensorboard logs will be stored\n",
        "# (if log_dir is not specified writer object will automatically generate filename)\n",
        "# Log files will be saved in 'experiments/exp-000'\n",
        "# create our custom logger\n",
        "logger = SummaryWriter(log_dir=osp.join(log_dir, name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzdzsBtgPTYw",
        "colab_type": "text"
      },
      "source": [
        "## Config initialization\n",
        "\n",
        "Here, let's use the project's parser with default parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj-W4GR_UOPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description=__doc__)\n",
        "\n",
        "parser.add_argument('--data-path', default='/datasets01/COCO/022719/', help='dataset')\n",
        "parser.add_argument('--dataset', default='coco', help='dataset')\n",
        "parser.add_argument('--model', default='maskrcnn_resnet50_fpn', help='model')\n",
        "parser.add_argument('--device', default='cuda', help='device')\n",
        "parser.add_argument('-b', '--batch-size', default=2, type=int,\n",
        "                    help='images per gpu, the total batch size is $NGPU x batch_size')\n",
        "parser.add_argument('--epochs', default=26, type=int, metavar='N',\n",
        "                    help='number of total epochs to run')\n",
        "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
        "                    help='number of data loading workers (default: 4)')\n",
        "parser.add_argument('--lr', default=0.02, type=float,\n",
        "                    help='initial learning rate, 0.02 is the default value for training '\n",
        "                          'on 8 gpus and 2 images_per_gpu')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                    help='momentum')\n",
        "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
        "                    metavar='W', help='weight decay (default: 1e-4)',\n",
        "                    dest='weight_decay')\n",
        "parser.add_argument('--lr-step-size', default=8, type=int, help='decrease lr every step-size epochs')\n",
        "parser.add_argument('--lr-steps', default=[16, 22], nargs='+', type=int, help='decrease lr every step-size epochs')\n",
        "parser.add_argument('--lr-gamma', default=0.1, type=float, help='decrease lr by a factor of lr-gamma')\n",
        "parser.add_argument('--print-freq', default=20, type=int, help='print frequency')\n",
        "parser.add_argument('--output-dir', default='.', help='path where to save')\n",
        "parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
        "parser.add_argument('--start_epoch', default=0, type=int, help='start epoch')\n",
        "parser.add_argument('--aspect-ratio-group-factor', default=3, type=int)\n",
        "parser.add_argument(\n",
        "    \"--test-only\",\n",
        "    dest=\"test_only\",\n",
        "    help=\"Only test the model\",\n",
        "    action=\"store_true\",\n",
        ")\n",
        "parser.add_argument(\n",
        "    \"--pretrained\",\n",
        "    dest=\"pretrained\",\n",
        "    help=\"Use pre-trained models from the modelzoo\",\n",
        "    action=\"store_true\",\n",
        ")\n",
        "\n",
        "# distributed training parameters\n",
        "parser.add_argument('--world-size', default=1, type=int,\n",
        "                    help='number of distributed processes')\n",
        "parser.add_argument('--dist-url', default='env://', help='url used to set up distributed training')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_OMSXLYP1KY",
        "colab_type": "text"
      },
      "source": [
        "Here we need to modify the default path with the path where the COCO **validation** dataset was unzipped. \n",
        "\n",
        "As the idea is to show how experiment management works in a reasonable time, we'll start with the pre-trained weights and will train the model on the validation set. As was mentioned before, **don't do this in real life as models should be trained on training images and validated on the validation set**. You'll see that metrics are increasing and it is expected because the model is trained and validated on the same data. We're doing this to save time! \n",
        "\n",
        "Also, we'll change learning rate to *0.0001* and increase batch size from *2* to *6*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aofDb-VcZmgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([\"--data-path\" ,'/content/coco', '--lr', \"1e-4\", '--pretrained', '--batch-size', \"6\"])\n",
        "config = vars(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wEcq5w1QKJi",
        "colab_type": "text"
      },
      "source": [
        "## Weights & Biases synchronization\n",
        "\n",
        "The last thing that we need to do before training is to connect to the Weights & Biases. Fortunately, we don't need to do much since it can be done by adding just two lines of code:\n",
        "\n",
        "1. The first line initializes experiment with config and experiment name\n",
        "2. The second line is used to enable TensorBoard synchronization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqsHS8rfbDYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.init(config=config, name=name)\n",
        "wandb.init(sync_tensorboard=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ7G6xx_Q4ov",
        "colab_type": "text"
      },
      "source": [
        "# Running train/val loop\n",
        "\n",
        "Here we go! Let's start the ball rolling. Please, visit [Weights & Biases](https://www.wandb.com/) to see how the experiment logging works in real-time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhKTNZEnUg_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(args)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}