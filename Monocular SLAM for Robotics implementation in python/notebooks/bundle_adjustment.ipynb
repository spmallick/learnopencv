{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446f75b2-65ab-4940-8430-c22b9ad80203",
   "metadata": {},
   "source": [
    "# Bundle adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010df0a3-5ba6-4d5f-b646-91f3d84de94c",
   "metadata": {},
   "source": [
    "This notebook visualizes the optimization of 3D points and camera poses using bundle adjustment.\n",
    "\n",
    "**Subjects covered**  \n",
    "1. **Definition of bundle adjustment.**\n",
    "2. **The g2o optimization library.**\n",
    "3. **Refining 3D point coordinates using bundle adjustment.**\n",
    "4. **Refining camera poses using bundle adjustment.**\n",
    "5. **Conclusion**\n",
    "6. **Sources**\n",
    "\n",
    "**Why are we interested in these subjects?**      \n",
    "Previous notebooks demonstrated algorithms for 3D point triangulation and camera pose optimization. These algorithms had **shortcomings**, as they were incapable of optimizing multiple points and poses simultaneously. The triangulation assumed perfect knowledge of poses. The PnP pose estimation assumed perfect knowledge of 3D points. Often we do not have perfect knowledge of either, we only have noisy data and initial guesses. Thus, we need an algorithm that can refine this noisy data to an optimal guess. Bundle adjustment achieves this. \n",
    "\n",
    "**Further reading**      \n",
    "For a comprehensive treatment of bundle adjustment, see chapter eighteen of [Multiple view geometry in computer vision](https://www.robots.ox.ac.uk/~vgg/hzbook/) by Richard Hartley and Andrew Zisserman. For a less thorough but more merciful introduction, see chapter seven of [Computer vision: algorithms and applications](https://szeliski.org/Book/) by Richard Szeliski and the [lectures and shorts](https://www.youtube.com/watch?v=lmj2Jk5tl60&ab_channel=CyrillStachniss) by Cyrill Stachniss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b7e5c9-c513-44ee-9cd6-135e679e05aa",
   "metadata": {},
   "source": [
    "## Bundle adjustment definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e183aa-8d6d-4d68-ae13-6f5e80774be6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Bundle adjustment refers to the problem of refining 3D point coordinates and camera poses using noisy 2D images of those points and initial camera pose estimates. Optionally, the camera intrinsics can also be optimized. In other words, it is an algorithm that determines the 3D map of a scene and the cameras' parameters within that scene. The solution is a map and cameras that best explain the observations, namely the cameras' images.\n",
    "\n",
    "Essentially, bundle adjustment is just an instance of a multi-parameter optimization problem. Similar to the problems of curve fitting and determining the weights of a neural network. As with those problems, bundle adjustment is concerned with minimizing an error metric. The error metric for bundle adjustment is the reprojection error. \n",
    "\n",
    "The reprojection error measures the in-image distance between the pixel point in the provided image, and the pixel point from the projected 3D world point. The projected 3D world point is calculated using the estimated camera pose and estimated 3D world point coordinate.\n",
    "\n",
    "<center>\n",
    "<img src=\"./images/reprojection_error.png\" width=400 />    \n",
    "<div align=\"center\">\n",
    "Image from <a href=\"https://support.pix4d.com/hc/en-us/articles/202559369-Reprojection-error\">Pix4D article</a>\n",
    "</div>\n",
    "</center>    \n",
    "\n",
    "\n",
    "This error can be expressed as a summation over all cameras and points    \n",
    "\n",
    "$$\\begin{align}\n",
    "E &= \\sum_{\\text{camera } \\in \\text{ cameras}} \\hspace{0.2em} \\sum_{<\\text{3D point, 2D point}> \\hspace{0.2em} \\in \\text{ points}} \\text{distance}(\\text{ 2D point}, \\text{ project}(\\text{ 3D point }, \\text{ camera})) \\\\\\\\\n",
    "E &= \\sum_{c \\hspace{0.1 em} \\in \\hspace{0.1 em} C} \\hspace{0.2em} \\sum_{p, \\hspace{0.1 em} p' \\hspace{0.1 em} \\in \\hspace{0.1 em} P} d(p', Q(p, c))) \n",
    "\\end{align}\n",
    "$$ \n",
    "\n",
    "For a large scene, not all points will be observed by all cameras. If a point is not observed, there is no reprojection error to minimize. Thus, it is useful to add a binary variable $v_{c, \\hspace{0.05 em} p}$ to ignore these non-interacting camera-point pairs.\n",
    "\n",
    "$$ E = \\sum_{c \\hspace{0.1 em} \\in \\hspace{0.1 em} C} \\hspace{0.2em} \\sum_{p, \\hspace{0.1 em} p' \\hspace{0.1 em} \\in \\hspace{0.1 em} P} v_{c,\\hspace{0.05 em} p}\\hspace{0.2 em} d(p', Q(p, c))) $$\n",
    "\n",
    "\n",
    "#### Multi-parameter optimization \n",
    "Bundle adjustment is a multi-parameter optimization problem. There are many multi-parameter optimization schemes one can choose from, such as: \n",
    "\n",
    "* iterative gradient descent \n",
    "* iterative least-squares (Gauss-Newton)\n",
    "* iterative gradient descent + least-squares (Levenberg-Marquardt)\n",
    "* interior-point methods\n",
    "\n",
    "The goal is to minimize the error by refining the camera poses and point coordinates. Assume we encode camera pose by a unit quaternion $q$ plus a translation vector $c$. Points are encoded as a 3-vector $p$. All these optimization algorithms would calculate the derivatives of $E$ w.r.t. the parameters: $\\frac{\\partial E}{\\partial q_{a}}$, $\\frac{\\partial E}{\\partial q_{b}}$, $\\frac{\\partial E}{\\partial q_{c}}$, $\\frac{\\partial E}{\\partial q_{d}}$, $\\frac{\\partial E}{\\partial c_{x}}$, $\\frac{\\partial E}{\\partial c_{y}}$, $\\frac{\\partial E}{\\partial c_{z}}$, $\\frac{\\partial E}{\\partial t_{x}}$, $\\frac{\\partial E}{\\partial t_{y}}$, $\\frac{\\partial E}{\\partial t_{z}}$. These derivatives would then inform an update rule that iteratively brings the parameters closer to a local optimum.\n",
    "\n",
    "**Levenberg-Marquardt** is often chosen in the bundle adjustment literature (see implementation and notes in notebook 2). This is because:\n",
    "* The dampening factor allows balancing between robust gradient descent and fast least-sqaures updates\n",
    "* There are many sparse implementations available for problems with sparse Jacobians\n",
    "\n",
    "Why would a sparse implementation be important? Well, applying Levenberg-Marquardt directly to large scenes with many points and cameras would be extremely computationally expensive. Take for example the derivative of the reprojection error w.r.t. a 3D point. Assume this derivative has to be calculated for $100$ thousand points. Now assume that there are $100$ cameras. That means $30$ million derivatives that need to be calculated at each optimization step. Most of these derivatives will be zero, as not all points will be observed by all cameras. Operating on these zeros and keeping them in memory is wasteful. The solution is a sparse implementation. The design of such a sparse algorithm is out of this notebook's scope. For a good summary of such a method, see the paper [SBA: A Software Package for Generic Sparse Bundle Adjustment](http://users.ics.forth.gr/~lourakis/sba/sba-toms.pdf) by Manolis Lourakis and Antonis Argyros with code available [here](https://github.com/balintfodor/sba)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc66458-ed02-45d2-b9b8-7b1289009232",
   "metadata": {},
   "source": [
    "## g2o optimization library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8312b295-9c9f-469a-ba14-92b1db1e56d0",
   "metadata": {},
   "source": [
    "The [g2o optimization library](https://github.com/RainerKuemmerle/g2o) is a framework for optimizing graph-based nonlinear error functions. Here, a graph is a general way to specify a system of coupled variables that produce the error. Formally, each vertex of the graph represents a state variable to optimize, each edge between two variables represents a pairwise\n",
    "observation of the two vertices it connects. For our bundle adjustment problem, the state variables are world points and camera poses. The edge that connects a point and a camera is the 2D observation of the point by the camera. \n",
    "\n",
    "The g2o library defines types for common types of state variables. They a listed here to avoid inline code comments. For cameras, there is the `VertexSCam`, where S stands for 'sparse'. For points, there is `VertexSBAPointXYZ`, which stands for 'Vertex Sparse Bundle Adjustment Point XYZ'. `VertexSCam` defines the camera pose as an `Isometry3d` type. An isometry is a distance-preserving transformation between metric spaces, i.e. a change of coordinate system. This makes sense, as we treat camera poses as coordinate systems in notebook 1. The edge connecting cameras and points is of type `Edge_XYZ_VSC`, where VSC stands for 'Vertex Sparse Camera'. Any mention of `SE3` refers to the mathematical group of rigid body motions, e.g. camera poses. \n",
    "\n",
    "We will use [g2opy](https://github.com/uoip/g2opy), a python binding of g2o. For more details on g2o, see the paper [g2o: A General Framework for Graph Optimization](http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle11icra.pdf). The code documentation is somewhat lacking. The best way to understand its structure is this [system description](https://github.com/uoip/g2opy/blob/master/doc/g2o.pdf) and reading the C++ [source code](https://github.com/RainerKuemmerle/g2o). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c40b46b-bbad-43d6-90a3-42720747cf94",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb79b220-1d9e-4b06-a5d5-1b359c2a5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_functions import (camera_centers, \n",
    "                                extrinsics, \n",
    "                                images, \n",
    "                                init_3d_plot, \n",
    "                                get_bunny, \n",
    "                                project_points_to_picture,\n",
    "                                intrinsics, \n",
    "                                plot_camera_wireframe, \n",
    "                                plot_picture, \n",
    "                                update_camera_wireframe, \n",
    "                                update_picture, \n",
    "                                distort_extrinsics, \n",
    "                                camera_centers_from_extrinsics)\n",
    "import ipyvolume as ipv\n",
    "import numpy as np \n",
    "import time \n",
    "import g2o "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d37b4-ff4b-4f6b-9ba1-fd114728df4b",
   "metadata": {},
   "source": [
    "## Set up a scene and distort bunny \n",
    "In this example, we will have perfect knowledge of the camera poses, perfect knowledge of observations, and extremely noisy estimates of 3D points. The goal is to recover the shape of the bunny through optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9339d184-cf2c-4d25-886e-9c3fa04983b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5f115e988f4cf8966c217d5a9fe62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(-1.1133407984528387, -0.9999999999999999, -…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis_scale = 5\n",
    "init_3d_plot()\n",
    "ipv.zlim(-35, 25)\n",
    "bunny_coords = get_bunny() \n",
    "b_xs, b_ys, b_zs = bunny_coords[:3]\n",
    "bunny_coords = np.vstack([bunny_coords, np.ones((1, bunny_coords.shape[1]))])\n",
    "distort_bunny_coords = bunny_coords + np.random.random(bunny_coords.shape) * 5\n",
    "b_xs, b_ys, b_zs, _ = distort_bunny_coords\n",
    "scatter_plot = ipv.scatter(b_xs, b_ys, b_zs, size=0.5, marker=\"sphere\", color='lime')\n",
    "for cam_center, extrinsic, image in zip(camera_centers, extrinsics, images):\n",
    "    image = np.zeros_like(image)\n",
    "    image = project_points_to_picture(image, bunny_coords, intrinsics, extrinsic)\n",
    "    inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic)\n",
    "    plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73c336-7d54-46ae-b98d-717a579f2921",
   "metadata": {},
   "source": [
    "## Define cameras, points, and observations as a g2o graph\n",
    "Here, we assume all points are visible in all cameras. As can be seen above, this is the case for this simple scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23eb4a29-cfea-486d-9005-a5bab24f5265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g2o graph has been created\n"
     ]
    }
   ],
   "source": [
    "# Sets optimizer \n",
    "optimizer = g2o.SparseOptimizer()\n",
    "solver = g2o.BlockSolverSE3(g2o.LinearSolverCSparseSE3())\n",
    "solver = g2o.OptimizationAlgorithmLevenberg(solver)\n",
    "optimizer.set_algorithm(solver)\n",
    "\n",
    "# Set real points in space\n",
    "true_points = bunny_coords[:3]\n",
    "distorted_points = distort_bunny_coords[:3]\n",
    "\n",
    "# Set camera intrinsics\n",
    "focal_length = (intrinsics[0, 0], intrinsics[1, 1])\n",
    "principal_point = ((intrinsics[0, 2], intrinsics[1, 2]))\n",
    "baseline = 0.0 # Not relevant, not a stereo setup\n",
    "K = np.array([*focal_length, *principal_point, baseline])\n",
    "g2o.VertexSCam.set_cam(*K)\n",
    "\n",
    "# Create vertices from cameras and fix them, as their pose is known. \n",
    "for idx_extr, extrinsic in enumerate(extrinsics):\n",
    "    pose = g2o.Isometry3d(np.linalg.inv(extrinsic))\n",
    "    v_se3 = g2o.VertexSCam()\n",
    "    v_se3.set_id(idx_extr)\n",
    "    v_se3.set_estimate(pose)\n",
    "    if idx_extr == 0:  # Fixing all cameras causes a segmentation fault, so unfix one\n",
    "        v_se3.set_fixed(False)\n",
    "    else:    \n",
    "        v_se3.set_fixed(True)\n",
    "    v_se3.set_all() # Calculates matrices related to projection\n",
    "    optimizer.add_vertex(v_se3)\n",
    "\n",
    "# Each vertex must have a unique id. \n",
    "# Ensure point and camera ids do not overlap.\n",
    "last_pose_id = idx_extr\n",
    "first_point_id = last_pose_id + 1\n",
    "\n",
    "# Create vertices from all points and connect them to the cameras\n",
    "for idx, (distorted_point, true_point) in enumerate(zip(distorted_points.T, true_points.T)):\n",
    "    point_id = idx + first_point_id\n",
    "    visible = []\n",
    "    \n",
    "    # Project the true points to the images, as observations are perfectly known\n",
    "    for ext_id in range(len(extrinsics)):\n",
    "        projected_point = optimizer.vertex(ext_id).map_point(true_point)\n",
    "        visible.append((ext_id, projected_point))\n",
    "    \n",
    "    # Create a point vertex with distorted estimate\n",
    "    vertex_point = g2o.VertexSBAPointXYZ()\n",
    "    vertex_point.set_id(point_id)\n",
    "    # see https://github.com/RainerKuemmerle/g2o/issues/109 on why we use this\n",
    "    vertex_point.set_marginalized(True)  \n",
    "    vertex_point.set_estimate(distorted_point)\n",
    "    optimizer.add_vertex(vertex_point)\n",
    "    \n",
    "    # Create an point-camera edge with each camera\n",
    "    for ext_idx, projected_point in visible:\n",
    "        edge = g2o.Edge_XYZ_VSC()\n",
    "        edge.set_vertex(0, vertex_point) \n",
    "        edge.set_vertex(1, optimizer.vertex(ext_idx)) \n",
    "        edge.set_measurement(projected_point) \n",
    "        # The information matrix is the measurement \n",
    "        # uncertainty. We set this equal for all measurements.\n",
    "        edge.set_information(np.identity(3))\n",
    "        edge.set_parameter_id(0, 0)\n",
    "        optimizer.add_edge(edge)\n",
    "    last_point_id = point_id\n",
    "\n",
    "optimizer.initialize_optimization()\n",
    "optimizer.set_verbose(False)\n",
    "print(\"g2o graph has been created\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fab0b-9845-4d94-b9da-57c049321e6c",
   "metadata": {},
   "source": [
    "## Visualize point optimization\n",
    "Visualize 50 steps of point refinement using sparse bundle adjustment optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51fd668-e7ca-455e-930a-340b2ca3af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5f115e988f4cf8966c217d5a9fe62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(aspect=1.6, fov=45.0, matrixWorldNeedsUpdate=True, position=(1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vertex_to_points(optimizer, first_point_id, last_point_id): \n",
    "    \"\"\" Converts g2o point vertices to their current 3d point estimates. \"\"\"\n",
    "    vertices_dict = optimizer.vertices()\n",
    "    estimated_points = list()\n",
    "    for idx in range(first_point_id, last_point_id): \n",
    "        estimated_points.append(vertices_dict[idx].estimate())\n",
    "    estimated_points = np.array(estimated_points)\n",
    "    xs, ys, zs = estimated_points.T\n",
    "    return xs, ys, zs \n",
    "\n",
    "ipv.show()\n",
    "for i in range(100):\n",
    "    optimizer.optimize(1)\n",
    "    xs, ys, zs = vertex_to_points(optimizer, first_point_id, last_point_id)\n",
    "    scatter_plot.x = xs \n",
    "    scatter_plot.y = ys \n",
    "    scatter_plot.z = zs \n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1bc8c-10db-4e24-b7d5-ce3c155c044b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up a scene and distort cameras \n",
    "In this example, we will have perfect knowledge of the bunny points, perfect knowledge of observations, and noisy estimates of camera positions. The goal is to recover camera positions through optimization. Ground truth camera positions are shown in red. Distorted camera positions are shown in blue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a24e5b2-37c7-4119-b76c-c72b557ac168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be458249c414490584bd5b8e3390593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=45.0, position=(-1.1133407984528387, -0.9999999999999999, -…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vertex_to_extrinsics(optimizer, last_pose_id):\n",
    "    \"\"\" Converts g2o camera vertices to their current 3d camera pose estimates. \"\"\"\n",
    "    vertices_dict = optimizer.vertices()\n",
    "    extrinsics = list()\n",
    "    for idx in range(last_pose_id + 1): \n",
    "        extrinsic = np.linalg.inv(vertices_dict[idx].estimate().matrix())\n",
    "        extrinsics.append(extrinsic)\n",
    "    return extrinsics\n",
    "\n",
    "distorted_extrinsics = [distort_extrinsics(extrinsic, max_angle=30, max_trans=5) for extrinsic in extrinsics]\n",
    "distorted_extrinsics[0] = extrinsics[0] # Do not distort the first camera\n",
    "distorted_centers = camera_centers_from_extrinsics(distorted_extrinsics)\n",
    "vis_scale = 5\n",
    "init_3d_plot()\n",
    "ipv.zlim(-35, 25)\n",
    "\n",
    "\n",
    "bunny_coords = get_bunny() \n",
    "b_xs, b_ys, b_zs = bunny_coords\n",
    "bunny_coords = np.vstack([bunny_coords, np.ones((1, bunny_coords.shape[1]))])\n",
    "scatter_plot = ipv.scatter(b_xs, b_ys, b_zs, size=0.5, marker=\"sphere\", color='lime')\n",
    "\n",
    "# Plot ground truth wireframes in red\n",
    "for cam_center, extrinsic in zip(camera_centers, extrinsics):\n",
    "    inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "    plot_camera_wireframe(cam_center, vis_scale, inv_extrinsic, color='blue')\n",
    "\n",
    "# Plot the distorted cameras  in blue\n",
    "extrinsics_estimates = vertex_to_extrinsics(optimizer, last_pose_id)\n",
    "old_plots = list()\n",
    "for distort_center, distort_extrinsic, true_extrinsic in zip(distorted_centers, distorted_extrinsics, extrinsics):\n",
    "    image = np.zeros_like(image)\n",
    "    image = project_points_to_picture(image, bunny_coords, intrinsics, true_extrinsic) \n",
    "    inv_extrinsic = np.linalg.pinv(distort_extrinsic)\n",
    "    wireframe = plot_camera_wireframe(distort_center, vis_scale, inv_extrinsic, color='red')\n",
    "    picture = plot_picture(image, inv_extrinsic, vis_scale)\n",
    "    old_plots.append([wireframe, picture])\n",
    "    \n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b05db-9a3e-48a7-bf4f-6d795a381dbb",
   "metadata": {},
   "source": [
    "## Define cameras, points, and observations as a g2o graph\n",
    "As with the previous problem, we define a g2o graph.\n",
    "Ideally, we would have been able to adapt the graph from the first problem and distort and unfix the cameras.\n",
    "Unfortunately, this leads to a segmentation fault. G2opy probably permanently sets values upon `optimizer.initialize_optimization()`. Thus, at least for now, redefining all vertices seems to be the only option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e7fcb3-ac19-4989-9a5b-9bfd199f0914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g2o graph has been created\n"
     ]
    }
   ],
   "source": [
    "optimizer = g2o.SparseOptimizer()\n",
    "solver = g2o.BlockSolverSE3(g2o.LinearSolverCSparseSE3())\n",
    "solver = g2o.OptimizationAlgorithmLevenberg(solver)\n",
    "optimizer.set_algorithm(solver)\n",
    "\n",
    "for idx_extr, extrinsic in enumerate(extrinsics):\n",
    "    # Initialize with true intrisics so the \n",
    "    # projection images will be correct. \n",
    "    # Distorted extrinsic estimates are set at the end \n",
    "    pose = g2o.Isometry3d(np.linalg.inv(extrinsic))\n",
    "    v_se3 = g2o.VertexSCam()\n",
    "    v_se3.set_id(idx_extr)\n",
    "    v_se3.set_estimate(pose)\n",
    "    if idx_extr:\n",
    "        v_se3.set_fixed(False)\n",
    "    else: \n",
    "        # Fix the first camera\n",
    "        v_se3.set_fixed(True)\n",
    "    v_se3.set_all()\n",
    "    optimizer.add_vertex(v_se3)\n",
    "    \n",
    "last_pose_id = idx_extr\n",
    "first_point_id = last_pose_id + 1\n",
    "\n",
    "for idx, true_point in enumerate(true_points.T):\n",
    "    point_id = idx + first_point_id\n",
    "    visible = []\n",
    "    # Use true extrinsics to project points\n",
    "    for ext_id in range(len(extrinsics)):\n",
    "        projected_point = optimizer.vertex(ext_id).map_point(true_point)\n",
    "        visible.append((ext_id, projected_point))\n",
    "    vertex_point = g2o.VertexSBAPointXYZ()\n",
    "    vertex_point.set_id(point_id)\n",
    "    vertex_point.set_marginalized(True)\n",
    "    vertex_point.set_estimate(true_point)\n",
    "    optimizer.add_vertex(vertex_point)\n",
    "    for ext_idx, projected_point in visible:\n",
    "        edge = g2o.Edge_XYZ_VSC()\n",
    "        edge.set_vertex(0, vertex_point) \n",
    "        edge.set_vertex(1, optimizer.vertex(ext_idx)) \n",
    "        edge.set_measurement(projected_point) \n",
    "        edge.set_information(np.identity(3))\n",
    "        edge.set_parameter_id(0, 0)\n",
    "        optimizer.add_edge(edge)\n",
    "    last_point_id = point_id\n",
    "\n",
    "# Set distorted extrinsics as pose estimate\n",
    "for idx_extr, distorted_extrinsic in enumerate(distorted_extrinsics):\n",
    "    if idx_extr: \n",
    "        pose = g2o.Isometry3d(np.linalg.inv(distorted_extrinsic))\n",
    "        optimizer.vertex(idx_extr).set_estimate(pose)\n",
    "        optimizer.vertex(idx_extr).set_all()\n",
    "\n",
    "optimizer.initialize_optimization()\n",
    "optimizer.set_verbose(False)\n",
    "print(\"g2o graph has been created\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e40b4b-9e1e-4ee9-929f-4bac5752c95f",
   "metadata": {},
   "source": [
    "## Visualize camera pose optimization\n",
    "Visualize 400 steps of camera pose refinement using sparse bundle adjustment optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b572b0b4-9463-46bb-8750-24c5adaa9ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be458249c414490584bd5b8e3390593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(aspect=1.6, fov=45.0, matrixWorldNeedsUpdate=True, position=(-0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ipv.show()\n",
    "time.sleep(2)\n",
    "for _ in range(50):\n",
    "    optimizer.optimize(2)\n",
    "    extrinsics_estimates = vertex_to_extrinsics(optimizer, last_pose_id)\n",
    "    new_plots = list()\n",
    "    for idx, (extrinsic, image) in enumerate(zip(extrinsics_estimates, images)):\n",
    "        wireframe, picture = old_plots[idx]\n",
    "        cam_center = -extrinsic[:3, :3].T @ extrinsic[:3, 3]\n",
    "        inv_extrinsic = np.linalg.pinv(extrinsic)\n",
    "        wireframe = update_camera_wireframe(cam_center, vis_scale, inv_extrinsic, wireframe)\n",
    "        new_picture = update_picture(image.copy(), inv_extrinsic.copy(), vis_scale, picture)\n",
    "        new_plots.append([wireframe, new_picture])\n",
    "    old_plots = new_plots\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb7087-11f7-4973-a047-d0e7d87eb785",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Excercise: Distort bunny, camera poses, and point projections\n",
    "In the previous cells, we refined the scene and camera locations in isolation.    \n",
    "As an exercise, create a graph where both camera estimates and scene point estimates are noisy.      \n",
    "One may even add noise to the coordinates of the projected points.     \n",
    "Judge the quality of the reconstruction: what is the mean squared error between estimated and true points?   \n",
    "Does the optimization require more iterations than when refining points or cameras in isolation?    \n",
    "How \"correct\" does the reconstruction look?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153f442-0eec-4620-98a9-6fbbd05b9cfa",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "In this notebook we:\n",
    "* Learned about bundle adjustment \n",
    "* Used bundle adjustment to refine scene points and camera locations\n",
    "\n",
    "Yes, Mr. Frodo, it's over now.    \n",
    "This concludes the notebook series.    \n",
    "I hope that you have learned something new about computer vision!   \n",
    "The reason I started this series was to gain an understanding of the methods that are involved in solving the SLAM problem.    \n",
    "For the next project, I would like to work on a well-documented implementation of [ORB-SLAM](https://courses.cs.washington.edu/courses/csep576/21au/resources/ORB-SLAM_A_Versatile_and_Accurate_Monocular_SLAM_System.pdf) written in C++, so stay \n",
    "tuned if that is of interest to you.   \n",
    "    \n",
    "Looking back at this project, I am quite pleased with the notebook visualization format.       \n",
    "In the future, I would like to continue this series to visualize more classical computer vision subjects, such as:      \n",
    "* Stereo rectification to create depth maps \n",
    "* Camera distortion parameters\n",
    "* Models of more extreme lenses, such as fisheye and wide-angle  lenses\n",
    "* Zhang's camera calibration algorithm\n",
    "* [Alternative camera models](https://www.microsoft.com/en-us/research/uploads/prod/2020/06/Why-Having-10000-Parameters-in-Your-Camera-Model-is-Better-Than-Twelve.pdf) with more parameters for greater accuracy and expressivity\n",
    "* Dense 3D reconstruction\n",
    "* Automatic camera calibration using heuristics and visual cues such as angles and straight lines\n",
    "* 3D point registration such as iterative closest point and derivatives\n",
    "* Classical optical flow\n",
    "* The bird's-eye view reprojection of a street as seen in some modern cars [[1]](https://img1.tongtool.com/r/ECEEHGKLCGEFNBEHCGIEMMILJLMJNGLILJGDX0Uk.jpg)[[2]](https://i.ytimg.com/vi/NQYxZ3DTToA/hqdefault.jpg)[[3]](https://img.alicdn.com/imgextra/i3/6000000005662/O1CN01Ph4xT31rhF17bkRxx_!!6000000005662-0-tbvideo.jpg)\n",
    "* Binary image features, ORB, FAST, and BRIEF\n",
    "* Modeling 3D lines using Plücker coordinates \n",
    "\n",
    "\n",
    "Good luck and stay well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5595c00-423e-49da-a726-34ba6a703aba",
   "metadata": {},
   "source": [
    "## Sources\n",
    "1. [g2o graph optimization framework](https://github.com/RainerKuemmerle/g2o)\n",
    "2. [Zisserman, Richard Hartley Andrew. \"Multiple view geometry in computer vision.\" 2004.](https://www.robots.ox.ac.uk/~vgg/hzbook/)\n",
    "3. [Szeliski, Richard. \"Computer vision: algorithms and applications.\" Springer Science & Business Media, 2010](https://szeliski.org/Book/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slam_env",
   "language": "python",
   "name": "slam_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
