{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50d65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-text-splitters langchain-community sentence-transformers pypdf langchain-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d3eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pdf2image qdrant-client pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9a72ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymupdf pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf920ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Imports\n",
    "import os\n",
    "from typing import List, Tuple, Any, Dict\n",
    "\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, pipeline\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Split PDF + Store with page metadata\n",
    "PDF_PATH = \"Dataset/QwenVL2.5.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents: List[Document] = loader.load()\n",
    "\n",
    "# Add page number to metadata if not present\n",
    "for i, doc in enumerate(documents):\n",
    "    if \"page\" not in doc.metadata:\n",
    "        doc.metadata[\"page\"] = i  # 0-based page index\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1600, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# In-memory Qdrant (you can switch to :memory: or remote URL)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "COLLECTION_NAME = \"pdf_rag\"\n",
    "if not client.collection_exists(COLLECTION_NAME):\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# ADD DOCUMENTS WITH PAGE METADATA\n",
    "# LangChain-Qdrant automatically stores metadata\n",
    "vector_store.add_documents(docs)\n",
    "\n",
    "# Create retriever (MMR for diversity)\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d7659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Qwen3-VL-4B-Instruct (GPU if available)\n",
    "def load_qwen_vl() -> Any:\n",
    "    try:\n",
    "        device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "        model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen3-VL-4B-Instruct\", \n",
    "            dtype=torch.bfloat16, \n",
    "            device_map=\"auto\")\n",
    "        \n",
    "        processor = AutoProcessor.from_pretrained(model_name)\n",
    "               \n",
    "        def qwen_vl(messages: List[Dict]) -> str:\n",
    "            nonlocal model  # Reference outer scope's model\n",
    "            \n",
    "            # Extract images and build proper message format\n",
    "            images = []\n",
    "            messages_processed = []\n",
    "            \n",
    "            for msg in messages:\n",
    "                new_msg = {\"role\": msg[\"role\"], \"content\": []}\n",
    "                if isinstance(msg.get(\"content\"), list):\n",
    "                    for item in msg[\"content\"]:\n",
    "                        if item.get(\"type\") == \"image\":\n",
    "                            images.append(item[\"image\"])\n",
    "                            new_msg[\"content\"].append({\"type\": \"image\"})\n",
    "                        elif item.get(\"type\") == \"text\":\n",
    "                            new_msg[\"content\"].append({\"type\": \"text\", \"text\": item[\"text\"]})\n",
    "                messages_processed.append(new_msg)\n",
    "            \n",
    "            # Use processor's apply_chat_template\n",
    "            text = processor.apply_chat_template(messages_processed, tokenize=False, add_generation_prompt=True)\n",
    "            \n",
    "            # Process with images and text\n",
    "            try:\n",
    "                inputs = processor(text=text, images=images if images else None, return_tensors=\"pt\")\n",
    "            except Exception as e:\n",
    "                print(f\"[Processor error] {e}. Trying text-only mode...\")\n",
    "                text_only = text.replace(\"<image>\", \"\").replace(\"<|image_|\", \"\").replace(\"|>\", \"\")\n",
    "                inputs = processor(text=text_only, return_tensors=\"pt\")\n",
    "            \n",
    "            # Move ALL tensors to correct device\n",
    "            if device == 0:\n",
    "                for key in inputs.keys():\n",
    "                    if isinstance(inputs[key], torch.Tensor):\n",
    "                        inputs[key] = inputs[key].to(\"cuda\")\n",
    "                if next(model.parameters()).device.type != 'cuda':\n",
    "                    print(\"[WARNING] Model not on CUDA, moving now...\")\n",
    "                    model = model.to(\"cuda\")\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            generated_ids = generated_ids[:, inputs[\"input_ids\"].shape[1]:]\n",
    "\n",
    "            response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            return response.strip()\n",
    "\n",
    "        return qwen_vl\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Qwen-VL] Load error: {e}\")\n",
    "        # Fallback tiny text model\n",
    "        fallback = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=\"google/flan-t5-small\",\n",
    "            max_length=256,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "        def fallback_vlm(messages: List[Dict]) -> str:\n",
    "            txt = \" \".join(\n",
    "                c[\"text\"] for m in messages for c in m[\"content\"] if c[\"type\"] == \"text\"\n",
    "            )\n",
    "            out = fallback(txt, do_sample=False)\n",
    "            return out[0][\"generated_text\"] if isinstance(out, list) else str(out)\n",
    "\n",
    "        return fallback_vlm\n",
    "\n",
    "\n",
    "vlm_pipeline = load_qwen_vl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def extract_text_and_page(item) -> Tuple[str, int]:\n",
    "    \"\"\"Works with LangChain Document or raw dict from Qdrant.\"\"\"\n",
    "    if isinstance(item, Document):\n",
    "        text = item.page_content\n",
    "        meta = item.metadata\n",
    "    elif isinstance(item, dict):\n",
    "        text = item.get(\"page_content\", str(item))\n",
    "        meta = item.get(\"metadata\", {})\n",
    "    else:\n",
    "        text = str(item)\n",
    "        meta = {}\n",
    "\n",
    "    page = meta.get(\"page\", -1)\n",
    "    if isinstance(page, str):\n",
    "        try:\n",
    "            page = int(page)\n",
    "        except ValueError:\n",
    "            page = -1\n",
    "    return text.strip(), page\n",
    "\n",
    "\n",
    "def get_page_images(pdf_path: str, page_numbers: List[int], max_pages: int = 5) -> List[Image.Image]:\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"[ERROR] PDF not found: {pdf_path}\")\n",
    "        return []\n",
    "\n",
    "    unique_pages = sorted({p for p in page_numbers if p >= 0})[:max_pages]\n",
    "    if not unique_pages:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        images = []\n",
    "\n",
    "        for page_num in page_numbers:\n",
    "            if page_num < 0 or page_num >= len(doc):\n",
    "                continue\n",
    "            page = doc[page_num]\n",
    "            pix = page.get_pixmap(dpi=100)  # REDUCED DPI from 200 to save memory\n",
    "            img = Image.frombytes(\"RGB\", (pix.width, pix.height), pix.samples)\n",
    "            images.append(img)\n",
    "\n",
    "        doc.close()\n",
    "        return images\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[PDF→Image] Error with PyMuPDF: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38de679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG + Multimodal Answer Function\n",
    "def rag_answer(query: str, task: str = None, k: int = 5, pdf_path: str = PDF_PATH, max_page_images: int = 2) -> str:\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return \"Error: PDF file not found.\"\n",
    "\n",
    "    # Retrieve\n",
    "    docs = None\n",
    "    try:\n",
    "        if hasattr(retriever, \"invoke\"):\n",
    "            docs = retriever.invoke(query)\n",
    "        elif hasattr(retriever, \"get_relevant_documents\"):\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "    except Exception:\n",
    "        docs = None\n",
    "\n",
    "    if docs is None:\n",
    "        try:\n",
    "            docs = vector_store.similarity_search(query, k=k)\n",
    "        except Exception:\n",
    "            docs = None\n",
    "\n",
    "    if not docs:\n",
    "        return \"No documents retrieved.\"\n",
    "\n",
    "    # Extract text + pages\n",
    "    text_page_pairs = [extract_text_and_page(d) for d in docs]\n",
    "    context_parts = [tp[0] for tp in text_page_pairs if tp[0]]\n",
    "    page_numbers = [tp[1] for tp in text_page_pairs if tp[1] >= 0]\n",
    "\n",
    "    if not context_parts:\n",
    "        return \"No usable text.\"\n",
    "\n",
    "    context = \"\\n---\\n\".join(context_parts)\n",
    "\n",
    "    # Get images (with reduced max_page_images)\n",
    "    page_images = get_page_images(pdf_path, page_numbers, max_pages=max_page_images)\n",
    "\n",
    "    # Task detection\n",
    "    if task is None:\n",
    "        q = query.lower()\n",
    "        if any(w in q for w in [\"summarize\", \"summary\", \"tl;dr\"]):\n",
    "            task = \"summarize\"\n",
    "        elif any(w in q for w in [\"ocr\", \"extract text\", \"read text\"]):\n",
    "            task = \"ocr\"\n",
    "        elif any(w in q for w in [\"what is\", \"describe\", \"what do you see\", \"vqa\"]):\n",
    "            task = \"vqa\"\n",
    "        elif any(w in q for w in [\"where is\", \"locate\", \"ground\", \"bbox\"]):\n",
    "            task = \"ground\"\n",
    "        else:\n",
    "            task = \"answer\"\n",
    "\n",
    "    # Build Qwen-VL messages\n",
    "    messages: List[Dict] = []\n",
    "\n",
    "    if page_images:\n",
    "        img_contents = [{\"type\": \"image\", \"image\": img} for img in page_images]\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": img_contents + [{\"type\": \"text\", \"text\": f\"Context (PDF pages {[p+1 for p in page_numbers]}):\\n{context}\\n\\n\"}]\n",
    "        })\n",
    "    else:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": f\"Context (text only):\\n{context}\\n\\n\"}]\n",
    "        })\n",
    "\n",
    "    # Task-specific instruction\n",
    "    if task == \"summarize\":\n",
    "        instruction = \"Summarize the document within 200 words\"\n",
    "    elif task == \"ocr\":\n",
    "        instruction = \"Extract **all readable text** from the provided page images. Return only plain text.\"\n",
    "    elif task == \"vqa\":\n",
    "        instruction = query\n",
    "    elif task == \"ground\":\n",
    "        instruction = (f\"Answer the question and, if you refer to objects in the images, \"\n",
    "            f\"return bounding boxes in format [x1,y1,x2,y2] (normalized 0–1).\\nQuestion: {query}\")\n",
    "    else:\n",
    "        instruction = (f\"Use the provided text context and images to answer concisely.\\n\"\n",
    "            f\"If the answer cannot be found, say *I don't know*.\\nQuestion: {query}\")\n",
    "\n",
    "    messages[-1][\"content\"].append({\"type\": \"text\", \"text\": instruction})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": []})\n",
    "\n",
    "    # Call VL model\n",
    "    try:\n",
    "        return vlm_pipeline(messages)\n",
    "    except Exception as e:\n",
    "        return f\"Model error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c356504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST IT!\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing retrieval...\")\n",
    "    print(retriever.invoke(\"Vision Encoder\"))\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG + Vision Answer:\")\n",
    "    answer = rag_answer(\n",
    "        query=\"Summarise Vision Encoder\",\n",
    "        task=\"summarize\",\n",
    "        k=5,\n",
    "        pdf_path=PDF_PATH,\n",
    "        max_page_images=1\n",
    "    )\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835aeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
