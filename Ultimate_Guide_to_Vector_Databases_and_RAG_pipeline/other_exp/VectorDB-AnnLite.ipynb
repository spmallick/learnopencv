{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ac21c8-838a-4b7d-b7e5-d5d333a15685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# %pip install langchain-text-splitters langchain-community langchain-openai langchain-chroma sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8768b4c",
   "metadata": {},
   "source": [
    "<!-- %pip install annlite -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a010cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662036c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650486b4-bcc5-45bd-853f-64dc691686e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pypdf transformers accelerate huggingface_hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3dc8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__);\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507d260-8c04-45c4-9cc2-485c56d10fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import AnnLite\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f88fb16-c87c-4929-ae8f-beb21fd489ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load document\n",
    "loader = PyPDFLoader(\"Dataset/Discoveries-and-Origin.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764c5fb-7489-4521-9c04-fc10bb25e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721c79e-b051-4daf-a2fc-47433e7f3721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5304\\2727068387.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# Embed and store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = AnnLite.from_documents(docs, embeddings, persist_directory=\"./annlite_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7358d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever init\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "globals()[\"retriever\"] = retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0293958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (837 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Ibrahim Effendi\n"
     ]
    }
   ],
   "source": [
    "# small LLM + RAG helper (flan-t5-small)\n",
    "hf_pipe = pipeline(\"text2text-generation\",  model=\"google/flan-t5-small\", max_length=256, device=0,)\n",
    "\n",
    "def smolLLM(prompt: str) -> str:\n",
    "    out = hf_pipe(prompt, do_sample=False)\n",
    "    # Transformers returns different shapes across versions\n",
    "    if isinstance(out, list) and len(out) > 0 and isinstance(out[0], dict):\n",
    "        return (\n",
    "            out[0].get(\"generated_text\")\n",
    "            or out[0].get(\"summary_text\")\n",
    "            or str(out[0])\n",
    "        )\n",
    "    return str(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84088a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust document-to-text extractor\n",
    "def extract_text(item):\n",
    "    \"\"\"Works with Document, (doc,score) tuples, dicts, etc.\"\"\"\n",
    "    try:\n",
    "        if hasattr(item, \"page_content\"):\n",
    "            return item.page_content\n",
    "        if isinstance(item, (list, tuple)) and len(item) > 0 and hasattr(item[0], \"page_content\"):\n",
    "            return item[0].page_content\n",
    "        if isinstance(item, dict) and \"page_content\" in item:\n",
    "            return item[\"page_content\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(item)\n",
    "\n",
    "# RAG answer function\n",
    "def rag_answer(query: str, k: int = 4) -> str:\n",
    "    docs = None\n",
    "\n",
    "    # retriever\n",
    "    try:\n",
    "        if \"retriever\" in globals() and hasattr(retriever, \"invoke\"):\n",
    "            docs = retriever.invoke(query)\n",
    "        elif \"retriever\" in globals() and hasattr(retriever, \"get_relevant_documents\"):\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "    except Exception:\n",
    "        docs = None\n",
    "\n",
    "    # db.similarity_search\n",
    "    if docs is None:\n",
    "        try:\n",
    "            if hasattr(db, \"similarity_search\"):\n",
    "                docs = db.similarity_search(query, k=k)\n",
    "        except Exception:\n",
    "            docs = None\n",
    "\n",
    "    # Build context\n",
    "    if not docs:\n",
    "        return \"No documents retrieved, cannot answer.\"\n",
    "\n",
    "    context_parts = [extract_text(d) for d in docs]\n",
    "    context = \"\\n---\\n\".join(context_parts)\n",
    "\n",
    "    # Prompt + call smolLLM\n",
    "    prompt = (\n",
    "        \"Use the following context to answer the question. \"\n",
    "        \"If the answer is not in the context, say *I don't know*.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer concisely:\"\n",
    "    )\n",
    "    return smolLLM(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bad3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Ibrahim Effendi\n"
     ]
    }
   ],
   "source": [
    "answer = rag_answer(\"Who invented the fire engine?\")\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a9dd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: ii.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_answer(\"Summarise Indigo section within 500 words\")\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f39b09-d5b1-4258-b24a-2ed090134474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The council of Nuremberg, it is said, prohibited the use of them in 1664, as is mentioned in the Hanau work, which I shall soon have occasion to quote. On the 24th of December, the same year, ribbon-looms were prohibited in the Spanish Netherlands.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_answer(\"Who prohibited use of ribbon-loom and when?\")\n",
    "print('Answer: ', answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
