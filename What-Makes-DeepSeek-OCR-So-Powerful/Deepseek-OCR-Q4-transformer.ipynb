{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039620dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85860661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install flash_attn-2.7.4%2Bcu124torch2.6.0cxx11abiFALSE-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508d0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55c74a-e270-4f96-8253-ed44b5d4649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install huggingface_hub transformers==4.46.3 tokenizers==0.20.3 einops addict easydict accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cf83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41828a83-1b0f-4a15-98ba-33edc891760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn version: 2.7.4\n",
      "Output shape: torch.Size([1, 8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"flash-attn version:\", __import__('flash_attn').__version__)\n",
    "\n",
    "q = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Output shape:\", out.shape)  # Should be [1, 8, 128, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8500e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\User\\.conda\\envs\\flashAttn2\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebbb2251cd9463eb1b01b47d733c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--Jalea96--DeepSeek-OCR-bnb-4bit-NF4. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7bac91b6cd4695b8d3d3d5ffbb0100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit model ready for inference!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Jalea96/DeepSeek-OCR-bnb-4bit-NF4\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    ),\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    ").eval()\n",
    "\n",
    "print(\"4-bit model ready for inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d04441",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c827c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n",
      "<|ref|>sub_title<|/ref|><|det|>[[48, 22, 260, 64]]<|/det|>\n",
      "## TinyBot  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[44, 95, 962, 236]]<|/det|>\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text- based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra- light neural core. Currently under development.  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[48, 306, 222, 333]]<|/det|>\n",
      "## Features  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[66, 364, 848, 557]]<|/det|>\n",
      "- Understands simple conversational commands- Controls virtual smart-home devices- Friendly, human-like responses- Built using TensorFlow- Fits within microcontroller memory constraints (few MBs)- Action generator to be added soon  \n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[48, 624, 565, 657]]<|/det|>\n",
      "## Supported Smart Devices  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[44, 685, 910, 740]]<|/det|>\n",
      "The bot can recognize and respond to queries about the following devices. More to be added soon.  \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[68, 761, 340, 988]]<|/det|>\n",
      "- Fan- Light & Dim Light- Curtain- Door Sensor- Refrigerator- Phone Charger- Aquarium Filter\n",
      "==================================================\n",
      "image size:  (548, 842)\n",
      "valid image tokens:  766\n",
      "output texts tokens (valid):  286\n",
      "compression ratio:  0.37\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 7/7 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944708a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashAttn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
