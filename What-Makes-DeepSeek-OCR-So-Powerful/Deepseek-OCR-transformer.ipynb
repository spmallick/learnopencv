{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbd50d3",
   "metadata": {},
   "source": [
    "# DeepSeek OCR "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892dc46",
   "metadata": {},
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a526dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "# %pip install flash_attn-2.7.4%2Bcu124torch2.6.0cxx11abiFALSE-cp310-cp310-win_amd64.whl\n",
    "# %pip install \"https://github.com/bitsandbytes-foundation/bitsandbytes/releases/download/continuous-release_main/bitsandbytes-1.33.7.preview-py3-none-win_amd64.whl\"\n",
    "# %pip install huggingface_hub transformers==4.46.3 tokenizers==0.20.3 einops addict easydict accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd821c",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82cf83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from flash_attn import flash_attn_func\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1da149",
   "metadata": {},
   "source": [
    "### Verify Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41828a83-1b0f-4a15-98ba-33edc891760e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash-attn version: 2.7.4\n",
      "Output shape: torch.Size([1, 8, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"flash-attn version:\", __import__('flash_attn').__version__)\n",
    "\n",
    "q = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "k = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "v = torch.randn(1, 8, 128, 64, device='cuda', dtype=torch.float16)\n",
    "\n",
    "out = flash_attn_func(q, k, v)\n",
    "print(\"Output shape:\", out.shape)  # Should be [1, 8, 128, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e4705e",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8500e1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29eb92b0a7554c0fa3c97bd8601852a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn2\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--deepseek-ai--DeepSeek-OCR. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947bb5c980c641eb83038e80e0ee979a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580ebef7eb6a4ddeafc6ca98ecd3dc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0358ce5404e94708a08b175158c5333c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d509cb146648978ff9e8c314fa2dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseekocr.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890b32c5b93a4c1f8cffbe7dee8905fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conversation.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- conversation.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac239125c28d4a2a9990749a569046f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseekv2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e31ad854ec493b862123206013d624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- configuration_deepseek_v2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- modeling_deepseekv2.py\n",
      "- configuration_deepseek_v2.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e3b966d126424381f2648dcc72ff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepencoder.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- deepencoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n",
      "- modeling_deepseekocr.py\n",
      "- conversation.py\n",
      "- modeling_deepseekv2.py\n",
      "- deepencoder.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f5406917c4b0283280aa308eb1290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9cf9df26f94c5cb7cbdbffa7b46861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c664dc1309e44ea9e6c31e0095ee17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name, \n",
    "    _attn_implementation='flash_attention_2', \n",
    "    trust_remote_code=True, \n",
    "    use_safetensors=True)\n",
    "\n",
    "model = model.eval().cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c827c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\.conda\\envs\\flashAttn2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "NO PATCHES\n",
      "=====================\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 78, 260, 222]]<|/det|>\n",
      "TinyBot \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[46, 333, 959, 806]]<|/det|>\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text-based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra-light neural core. Currently under development.\n",
      "==================================================\n",
      "image size:  (548, 250)\n",
      "valid image tokens:  116\n",
      "output texts tokens (valid):  96\n",
      "compression ratio:  0.83\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 8.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './github-png-readme.md'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7944708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "NO PATCHES\n",
      "=====================\n",
      "<|ref|>sub_title<|/ref|><|det|>[[46, 78, 260, 222]]<|/det|>\n",
      "TinyBot \n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[46, 333, 959, 806]]<|/det|>\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text-based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra-light neural core. Currently under development.\n",
      "==================================================\n",
      "image size:  (548, 250)\n",
      "valid image tokens:  116\n",
      "output texts tokens (valid):  96\n",
      "compression ratio:  0.83\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 2/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 5.61 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './output'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfd36afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "NO PATCHES\n",
      "=====================\n",
      "# TinyBot  \n",
      "\n",
      "Built to decide smart home actions intelligently, without explicit hard coding. TinyBot is a minimal, text-based conversational model designed to mimic basic understanding of home environment and your needs. It is similar to a chatbot but with an ultra-light neural core. Currently under development.\n",
      "==================================================\n",
      "image size:  (548, 250)\n",
      "valid image tokens:  116\n",
      "output texts tokens (valid):  59\n",
      "compression ratio:  0.51\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|ocr|>Convert the document to markdown. \"\n",
    "image_file = '../../tasks/github-readme.png'\n",
    "output_path = './output2'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([8, 100, 1280])\n",
      "=====================\n",
      "# Gemma 2: Improving Open Language Models at a Practical Size  \n",
      "\n",
      "Gemma Team, Google DeepMind\\(^1\\)  \n",
      "\n",
      "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023). We also train the 2B and 9B models with knowledge distillation (Hinton et al., 2015) instead of next token prediction. The resulting models deliver the best performance for their size, and even offer competitive alternatives to models that are 2-3× bigger. We release all our models to the community.\n",
      "==================================================\n",
      "image size:  (1920, 1080)\n",
      "valid image tokens:  944\n",
      "output texts tokens (valid):  191\n",
      "compression ratio:  0.2\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 11.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|ocr|>Convert the document to markdown.\"\n",
    "image_file = '../../tasks/ocr.png'\n",
    "output_path = './output2'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63d0125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([8, 100, 1280])\n",
      "=====================\n",
      "<table><tr><td>SL NO</td><td>A</td><td>Particulars</td><td>Qty</td><td>Make</td></tr><tr><td>1</td><td>32A MCB 4P, C Curve</td><td></td><td>1</td><td>L&K</td></tr><tr><td>2</td><td>Voltage Monitoring Relay</td><td></td><td>1</td><td>L&K</td></tr><tr><td>3</td><td>40A Contactor 4P, MCX-04</td><td></td><td>1</td><td>L&K</td></tr><tr><td>4</td><td>Red LVGP Indicator</td><td></td><td>1</td><td>ESBEE</td></tr><tr><td>5</td><td>Green LVGP Indicator</td><td></td><td>1</td><td>ESBEE</td></tr><tr><td>6</td><td>Blue LVGP Indicator</td><td></td><td>1</td><td>ESBEE</td></tr><tr><td>7</td><td>6A Fuse Terminal</td><td></td><td>3</td><td>FTC</td></tr><tr><td>8</td><td>Emergency Push Button Stay Put, Rotate to Reset with NC Element</td><td></td><td>1</td><td>C&S</td></tr></table>\n",
      "==================================================\n",
      "image size:  (1920, 1080)\n",
      "valid image tokens:  944\n",
      "output texts tokens (valid):  217\n",
      "compression ratio:  0.23\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 12.83 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|ocr|>Convert the document to markdown.\"\n",
    "image_file = '../../tasks/table.png'\n",
    "output_path = './output3'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f516cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 100, 1280])\n",
      "=====================\n",
      "# IEC 61439: Alternate Design Verification Methods\n",
      "\n",
      "N. S. Vijayanarayanan, Kushal Parwal and Ravindra Kadam  \n",
      "Larsen & Toubro Ltd.\n",
      "\n",
      "## Abstract\n",
      "\n",
      "Design verification for low-voltage power switchgear and control gear assemblies is intended to verify compliance of the design of an assembly or assembly system with the requirements of IEC61439 series of standards. There are 3 methods of verification. 1. Verification testing 2. Verification comparison with a tested reference design 3. Verification assessment by calculations and design rules including use of appropriate safety margins. The normally preferred verification method is verification by testing. However, verification by assessment and verification by comparison are also alternate verification methods provided by IEC 61439, which are still unexplored. This is in spite of the fact that IEC 61439 states: “all the permitted means of design verification which includes comparison and assessment are equivalent in terms of performance achieved.” Here, we shall elaborate on the verification assessment and verification comparison methods for Low voltage switchgear and control gear assemblies.\n",
      "\n",
      "**Keywords:** Assessment, Assembly, Comparison, Low Voltage Switchgear, Testing, Verification\n",
      "\n",
      "## 1. Verification Assessment\n",
      "\n",
      "Design verification by strict design rules or calculations applied to a sample of an assembly or to parts of assemblies to show that the design meets the requirements of the relevant assembly standard.\n",
      "\n",
      "When there is more than one method for the same verification, they are considered equivalent and the selection of the appropriate method is the responsibility of the original manufacturer.\n",
      "\n",
      "However, all the tests are not possible with assessment. Annex D of IEC 61439-1 mentions the verification options available and applicable against each tests.\n",
      "\n",
      "Let’s discuss about the tests which are possible by assessment.\n",
      "\n",
      "### 1.1 Glow Wire Test\n",
      "\n",
      "**Verification of resistance of insulating materials to abnormal heat and fire due to internal electric effects:**\n",
      "\n",
      "- 960°C for parts necessary to retain current-carrying parts in position.\n",
      "- 850°C for enclosures intended for mounting in hollow walls.\n",
      "\n",
      "The tests are conducted by the following methods:\n",
      "\n",
      "- 96°C for parts necessary to retain current-carrying parts in position.\n",
      "- 850° C for enclosures intended for mounting in hollow walls.\n",
      "\n",
      "## 2. Verification Comparison\n",
      "\n",
      "### 2.1 Temperature Rise\n",
      "\n",
      "Standard defines how the rated currents of variants can be verified by derivation from similar arrangements verified by test.\n",
      "\n",
      "1) Temperature-rise tests on the circuit(s) carried out at 50 Hz are applicable to 60 Hz for rated currents up to and including 800A. In the absence of tests at 60 Hz for currents above 800A, the rated current at 60 Hz shall be reduced to 95% of that at 50 Hz. Alternatively, where the maximum temperature rise at 50 Hz does not exceed 90% of the permissible value, then derating for 60 Hz is not required. Tests carried out at a particular frequency are applicable at the same current rating to lower frequencies including d.c.\n",
      "\n",
      "2) Assemblies verified by derivation from a similar tested arrangement shall comply with the following:\n",
      "\n",
      "a) The functional units shall belong to the same group as the functional unit selected for test\n",
      "\n",
      "b) The same type of construction as used for the test\n",
      "\n",
      "c) The same or increased overall dimensions as used for the test\n",
      "\n",
      "d) The same or increased cooling conditions as used for the test (forced or natural convection, same or larger ventilation openings)\n",
      "\n",
      "e) The same or reduced internal separations as used for the test (if any)\n",
      "\n",
      "f) The same or reduced power losses in the same section as used for the test\n",
      "\n",
      "3) Thermal tests performed on 3-phase, 3-wire assemblies are considered as representing 3-phase, 4-wire and single-phase, 2-wire or 3-wire assemblies, provided that the neutral conductor is sized equal to or greater than the phase conductors arranged in the same manner.\n",
      "\n",
      "4) Busbars: Ratings established for aluminum busbars are valid for copper Busbars with the same cross sectional dimensions and configuration. However, ratings established for copper busbars shall not be used to establish ratings of aluminum busbars.\n",
      "\n",
      "4) The ratings of variants not selected for test shall be determined by multiplying their cross-section with the current density of a larger cross-section busbar of the same design that has been verified by test.\n",
      "\n",
      "5) If additionally, a similar cross-section than the one to be derived has been tested, which also fulfils the conditions, then the rating of the intermediate variants may be established by interpolation.\n",
      "\n",
      "6) The standard allows, in clearly defined circumstances, for the derivation of rating of a double lamination busbar has been established by test, it is acceptable to assign a rating equal to 50% of the tested arrangement to a busbar comprising a single lamination with the same width and thickness as the tested laminations, when all other considerations are the same.\n",
      "\n",
      "**Functional units - Device substitution**\n",
      "\n",
      "A device may be substituted with a similar device from another series to that used in the original verification, provided that the power loss and terminal temperature rise of the device, t=when tested in accordance with its product standard, is the same or lower. In addition, the physical arrangement within the functional unit and the rating of the functional unit shall be maintained.\n",
      "\n",
      "### 2.2 Short Circuit\n",
      "\n",
      "Short circuit verification by comparison can be done in two ways\n",
      "\n",
      "a) Using a check list:\n",
      "\n",
      "| Item No. | Requirements to be considered |\n",
      "|---------|-------------------------------|\n",
      "| 1       | Is the short-circuit withstand rating of each circuit of the assembly to be assessed, less than or equal to, that of the reference design? |\n",
      "| 2       | Is the cross-sectional dimensions of the busbars and connections of each circuit of the assembly to be assessed, greater than or equal to, those of the reference design? |\n",
      "| 3       | Is the center line spacing of the busbars and connections of each circuit of the assembly to be assessed, greater than or equivalent to, those of the reference design? |\n",
      "| 4       | Are the busbar supports of each circuit of the assembly to be assessed of the same type, shape and material and have, the same or smaller center line spacing, along the length of the busbar as the reference design? And is the mounting structure for the busbar supports of the same design and mechanical strength? |\n",
      "\n",
      "**Verification of resistance of insulating materials to abnormal heat and fire due to internal electric effects:**\n",
      "\n",
      "1) 960°C for parts necessary to retain current-carrying parts in position.\n",
      "\n",
      "2) 850°C for enclosures intended for mounting in hollow walls.\n",
      "\n",
      "**Author for correspondence**\n",
      "\n",
      "© Power Research\n",
      "\n",
      "30 | Vol 16(1) | January-June 2020\n",
      "\n",
      "www.cpjrjournal.in\n",
      "==================================================\n",
      "image size:  (3000, 4000)\n",
      "valid image tokens:  792\n",
      "output texts tokens (valid):  1386\n",
      "compression ratio:  1.75\n",
      "==================================================\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 67.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"<image>\\n<|ocr|>Convert the document to markdown.\"\n",
    "image_file = '../../tasks/multi-page-pdf.png'\n",
    "output_path = './output4'\n",
    "\n",
    "t1 = time.time()\n",
    "res = model.infer(tokenizer, \n",
    "                  prompt=prompt, \n",
    "                  image_file=image_file, \n",
    "                  output_path = output_path, \n",
    "                  base_size = 1024, \n",
    "                  image_size = 640, \n",
    "                  crop_mode=True, \n",
    "                  save_results = True, \n",
    "                  test_compress = True)\n",
    "t2 = time.time()\n",
    "print(f\"Time taken: {round(t2-t1, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842cfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flashAttn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
